
==> Audit <==
|-----------|------------------------|----------|--------------------------------|---------|---------------------|---------------------|
|  Command  |          Args          | Profile  |              User              | Version |     Start Time      |      End Time       |
|-----------|------------------------|----------|--------------------------------|---------|---------------------|---------------------|
| dashboard |                        | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 12 Dec 24 18:29 IST |                     |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| start     |                        | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 12 Dec 24 18:29 IST | 12 Dec 24 18:35 IST |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| dashboard |                        | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 12 Dec 24 18:35 IST |                     |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| service   | myapp                  | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 12 Dec 24 21:02 IST | 12 Dec 24 21:07 IST |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| dashboard |                        | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 12 Dec 24 21:07 IST |                     |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| delete    |                        | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 12 Dec 24 23:01 IST | 12 Dec 24 23:01 IST |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| start     |                        | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 12 Dec 24 23:01 IST |                     |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| start     |                        | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 27 Dec 24 16:29 IST | 27 Dec 24 16:29 IST |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| service   | dashboard              | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 27 Dec 24 16:30 IST |                     |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| dashboard |                        | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 27 Dec 24 16:30 IST |                     |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| service   | my-app                 | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 27 Dec 24 16:51 IST |                     |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| service   | my-app                 | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 27 Dec 24 16:52 IST |                     |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| service   | my-app                 | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 27 Dec 24 16:54 IST |                     |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| dashboard |                        | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 27 Dec 24 16:55 IST |                     |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| delete    |                        | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 27 Dec 24 16:58 IST | 27 Dec 24 16:58 IST |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| start     |                        | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 27 Dec 24 16:59 IST | 27 Dec 24 17:00 IST |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| service   | mu-nginx               | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 27 Dec 24 17:10 IST |                     |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| service   | my-nginx               | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 27 Dec 24 17:10 IST | 27 Dec 24 17:11 IST |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| service   | my-nginx               | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 27 Dec 24 17:11 IST | 27 Dec 24 17:11 IST |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| dashboard |                        | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 27 Dec 24 17:11 IST |                     |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| delete    |                        | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 28 Dec 24 01:31 IST | 28 Dec 24 01:31 IST |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| start     |                        | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 04 Jan 25 01:36 IST | 04 Jan 25 01:38 IST |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| service   | service.yml            | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 04 Jan 25 01:40 IST |                     |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| service   | static-web-app-service | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 04 Jan 25 01:42 IST |                     |
|           |                        |          | Dhapodkar                      |         |                     |                     |
| service   | static-web-app-service | minikube | DESKTOP-AUBMJ63\Payal          | v1.33.0 | 04 Jan 25 01:44 IST |                     |
|           |                        |          | Dhapodkar                      |         |                     |                     |
|-----------|------------------------|----------|--------------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/01/04 01:36:51
Running on machine: DESKTOP-AUBMJ63
Binary: Built with gc go1.22.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0104 01:36:51.679332   26544 out.go:291] Setting OutFile to fd 84 ...
I0104 01:36:51.680332   26544 out.go:343] isatty.IsTerminal(84) = true
I0104 01:36:51.680332   26544 out.go:304] Setting ErrFile to fd 88...
I0104 01:36:51.680332   26544 out.go:343] isatty.IsTerminal(88) = true
I0104 01:36:51.739264   26544 out.go:298] Setting JSON to false
I0104 01:36:51.750990   26544 start.go:129] hostinfo: {"hostname":"DESKTOP-AUBMJ63","uptime":1992097,"bootTime":1733942713,"procs":324,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.5247 Build 19045.5247","kernelVersion":"10.0.19045.5247 Build 19045.5247","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"e304e835-f6bd-420f-b016-608a520bc9b8"}
W0104 01:36:51.751165   26544 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0104 01:36:51.761978   26544 out.go:177] 😄  minikube v1.33.0 on Microsoft Windows 10 Pro 10.0.19045.5247 Build 19045.5247
I0104 01:36:51.768165   26544 notify.go:220] Checking for updates...
I0104 01:36:51.770178   26544 driver.go:392] Setting default libvirt URI to qemu:///system
I0104 01:36:51.771178   26544 global.go:112] Querying for installed drivers using PATH=C:\Program Files\OpenLogic\jdk-17.0.11.9-hotspot\bin;C:\Program Files\Microsoft SDKs\Azure\CLI2\wbin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Payal Dhapodkar\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Python 3.8;C:\Program Files\Amazon\AWSCLIV2\;C:\ProgramData\chocolatey\bin;C:\Python34\Scripts\pip;C:\Users\Payal Dhapodkar\AppData\Local\Programs\Python\Python38\Scripts;C:\Program Files\Go\bin;C:\ProgramData\Microsoft\Windows\Start Menu\Programs\Node.js;C:\Program Files (x86)\Pulse Secure\VC142.CRT\X64\;C:\Program Files (x86)\Pulse Secure\VC142.CRT\X86\;C:\Users\Payal Dhapodkar\AppData\Local\Programs\Git\usr\bin;C:\Users\Payal Dhapodkar\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Python 3.8;C:\Users\Payal Dhapodkar\AppData\Local\Programs\Python\Python38;C:\Program Files\PuTTY\;C:\Program Files\Amazon\AWSCLIV2\awscli;C:\Windows\System32\cmd.exe;C:\Users\Payal Dhapodkar;C:\Users\Payal Dhapodkar\AppData\Roaming\vault_1.15.1_windows_386;C:\Program Files\Java\jdk-22\bin;C:\Users\Admin\AppData\Roaming\nvm;C:\Program Files\nodejs;C:\ProgramData\Microsoft\Windows\Start Menu\Programs\NVM for Windows;C:\Program Files\Docker\Docker\resources\bin;C:\Program Files\Kubernetes\Minikube;C:\Program Files (x86)\Java\jre-1.8\bin;C:\Program Files (x86)\apache-tomcat-7.0.109\bin;C:\Users\Payal Dhapodkar\Downloads\apache-tomcat-7.0.109-windows-x64\apache-tomcat-7.0.109\bin;C:\Program Files\apache-maven-3.9.9\bin;C:\Program Files\OpenLogic\jdk-17.0.11.9-hotspot\bin;C:\Users\Payal Dhapodkar\AppData\Local\Microsoft\WindowsApps;C:\Users\Payal Dhapodkar\AppData\Local\Programs\Microsoft VS Code\bin;C:\Users\Payal Dhapodkar\AppData\Local\Programs\Git\cmd;C:\Users\Payal Dhapodkar\AppData\Roaming\npm;C:\Users\Payal Dhapodkar\go\bin;C:\Program Files (x86)\MongoDB Atlas CLI\;C:\Users\Admin\AppData\Roaming\nvm;C:\Program Files\nodejs
I0104 01:36:51.792830   26544 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0104 01:36:52.067334   26544 docker.go:122] docker version: linux-26.0.0:Docker Desktop 4.29.0 (145265)
I0104 01:36:52.083037   26544 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0104 01:36:52.296367   26544 lock.go:35] WriteFile acquiring C:\Users\Payal Dhapodkar\.minikube\last_update_check: {Name:mk1e2e706e35a2d6355a509f16efc5d014bfe809 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 01:36:52.301429   26544 out.go:177] 🎉  minikube 1.34.0 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.34.0
I0104 01:36:52.304579   26544 out.go:177] 💡  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0104 01:36:53.339604   26544 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.2559711s)
I0104 01:36:53.343666   26544 info.go:266] docker info: {ID:3c9c3cc6-4a23-4808-884e-07ea7a5d507b Containers:46 ContainersRunning:34 ContainersPaused:0 ContainersStopped:12 Images:24 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:175 OomKillDisable:true NGoroutines:178 SystemTime:2025-01-03 20:06:53.277580982 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8193646592 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.26.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.27] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.6.3]] Warnings:<nil>}}
I0104 01:36:53.343666   26544 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0104 01:36:53.371409   26544 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0104 01:36:53.371409   26544 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0104 01:36:55.862316   26544 global.go:133] hyperv default: true priority: 8, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0104 01:36:55.880206   26544 global.go:133] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0104 01:36:55.917714   26544 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0104 01:36:55.919478   26544 driver.go:314] not recommending "ssh" due to default: false
I0104 01:36:55.919478   26544 driver.go:349] Picked: docker
I0104 01:36:55.919478   26544 driver.go:350] Alternatives: [hyperv ssh]
I0104 01:36:55.919478   26544 driver.go:351] Rejects: [vmware podman qemu2 virtualbox]
I0104 01:36:55.921688   26544 out.go:177] ✨  Automatically selected the docker driver. Other choices: hyperv, ssh
I0104 01:36:55.926522   26544 start.go:297] selected driver: docker
I0104 01:36:55.926522   26544 start.go:901] validating driver "docker" against <nil>
I0104 01:36:55.926522   26544 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0104 01:36:55.960401   26544 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0104 01:36:56.371227   26544 info.go:266] docker info: {ID:3c9c3cc6-4a23-4808-884e-07ea7a5d507b Containers:46 ContainersRunning:34 ContainersPaused:0 ContainersStopped:12 Images:24 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:175 OomKillDisable:true NGoroutines:178 SystemTime:2025-01-03 20:06:56.337584951 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8193646592 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.26.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.27] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.6.3]] Warnings:<nil>}}
I0104 01:36:56.371297   26544 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0104 01:36:56.502785   26544 start_flags.go:393] Using suggested 4000MB memory alloc based on sys=16118MB, container=7814MB
I0104 01:36:56.503599   26544 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0104 01:36:56.507123   26544 out.go:177] 📌  Using Docker Desktop driver with root privileges
I0104 01:36:56.512135   26544 cni.go:84] Creating CNI manager for ""
I0104 01:36:56.512135   26544 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0104 01:36:56.512135   26544 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0104 01:36:56.513131   26544 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Payal Dhapodkar:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0104 01:36:56.519128   26544 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0104 01:36:56.524163   26544 cache.go:121] Beginning downloading kic base image for docker with docker
I0104 01:36:56.527145   26544 out.go:177] 🚜  Pulling base image v0.0.43 ...
I0104 01:36:56.531900   26544 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0104 01:36:56.531900   26544 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon
I0104 01:36:56.533701   26544 preload.go:147] Found local preload: C:\Users\Payal Dhapodkar\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0104 01:36:56.534282   26544 cache.go:56] Caching tarball of preloaded images
I0104 01:36:56.534841   26544 preload.go:173] Found C:\Users\Payal Dhapodkar\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0104 01:36:56.535349   26544 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0104 01:36:56.536029   26544 profile.go:143] Saving config to C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\config.json ...
I0104 01:36:56.536576   26544 lock.go:35] WriteFile acquiring C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\config.json: {Name:mk4f163a64daca7e1ab7e907a0054fde34b8cc6f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 01:36:56.773439   26544 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon, skipping pull
I0104 01:36:56.773439   26544 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 exists in daemon, skipping load
I0104 01:36:56.773946   26544 cache.go:194] Successfully downloaded all kic artifacts
I0104 01:36:56.777004   26544 start.go:360] acquireMachinesLock for minikube: {Name:mka1fd342438840b0bc0fcb5144870905c58973d Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0104 01:36:56.778046   26544 start.go:364] duration metric: took 1.0414ms to acquireMachinesLock for "minikube"
I0104 01:36:56.778046   26544 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Payal Dhapodkar:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0104 01:36:56.778046   26544 start.go:125] createHost starting for "" (driver="docker")
I0104 01:36:56.781687   26544 out.go:204] 🔥  Creating docker container (CPUs=2, Memory=4000MB) ...
I0104 01:36:56.787067   26544 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0104 01:36:56.787067   26544 client.go:168] LocalClient.Create starting
I0104 01:36:56.790507   26544 main.go:141] libmachine: Reading certificate data from C:\Users\Payal Dhapodkar\.minikube\certs\ca.pem
I0104 01:36:56.791564   26544 main.go:141] libmachine: Decoding PEM data...
I0104 01:36:56.791593   26544 main.go:141] libmachine: Parsing certificate...
I0104 01:36:56.792674   26544 main.go:141] libmachine: Reading certificate data from C:\Users\Payal Dhapodkar\.minikube\certs\cert.pem
I0104 01:36:56.793298   26544 main.go:141] libmachine: Decoding PEM data...
I0104 01:36:56.793298   26544 main.go:141] libmachine: Parsing certificate...
I0104 01:36:56.816197   26544 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0104 01:36:57.098863   26544 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0104 01:36:57.123180   26544 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0104 01:36:57.123540   26544 cli_runner.go:164] Run: docker network inspect minikube
W0104 01:36:57.379296   26544 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0104 01:36:57.379296   26544 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0104 01:36:57.379296   26544 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0104 01:36:57.404377   26544 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0104 01:36:57.765902   26544 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc002520de0}
I0104 01:36:57.765902   26544 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0104 01:36:57.798577   26544 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0104 01:36:58.185705   26544 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0104 01:36:58.186225   26544 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0104 01:36:58.237656   26544 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0104 01:36:58.507892   26544 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0104 01:36:58.809266   26544 oci.go:103] Successfully created a docker volume minikube
I0104 01:36:58.826266   26544 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib
I0104 01:37:01.041272   26544 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib: (2.2150062s)
I0104 01:37:01.041272   26544 oci.go:107] Successfully prepared a docker volume minikube
I0104 01:37:01.041272   26544 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0104 01:37:01.041272   26544 kic.go:194] Starting extracting preloaded images to volume ...
I0104 01:37:01.060266   26544 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v "C:\Users\Payal Dhapodkar\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro" -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir
I0104 01:37:22.334239   26544 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v "C:\Users\Payal Dhapodkar\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro" -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir: (21.2739728s)
I0104 01:37:22.334239   26544 kic.go:203] duration metric: took 21.2929671s to extract preloaded images to volume ...
I0104 01:37:22.353669   26544 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0104 01:37:22.825159   26544 info.go:266] docker info: {ID:3c9c3cc6-4a23-4808-884e-07ea7a5d507b Containers:46 ContainersRunning:34 ContainersPaused:0 ContainersStopped:12 Images:24 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:175 OomKillDisable:true NGoroutines:178 SystemTime:2025-01-03 20:07:22.794357378 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8193646592 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.26.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.27] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.6.3]] Warnings:<nil>}}
I0104 01:37:22.838452   26544 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0104 01:37:23.301356   26544 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=4000mb --memory-swap=4000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737
I0104 01:37:24.246750   26544 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0104 01:37:24.492497   26544 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0104 01:37:24.708516   26544 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0104 01:37:25.014556   26544 oci.go:144] the created container "minikube" has a running status.
I0104 01:37:25.015030   26544 kic.go:225] Creating ssh key for kic: C:\Users\Payal Dhapodkar\.minikube\machines\minikube\id_rsa...
I0104 01:37:25.118355   26544 kic_runner.go:191] docker (temp): C:\Users\Payal Dhapodkar\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0104 01:37:25.753226   26544 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0104 01:37:26.104694   26544 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0104 01:37:26.104694   26544 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0104 01:37:26.360052   26544 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\Payal Dhapodkar\.minikube\machines\minikube\id_rsa...
I0104 01:37:27.386247   26544 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0104 01:37:27.600574   26544 machine.go:94] provisionDockerMachine start ...
I0104 01:37:27.622841   26544 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 01:37:27.853154   26544 main.go:141] libmachine: Using SSH client type: native
I0104 01:37:27.856166   26544 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10fa1c0] 0x10fcda0 <nil>  [] 0s} 127.0.0.1 51321 <nil> <nil>}
I0104 01:37:27.856166   26544 main.go:141] libmachine: About to run SSH command:
hostname
I0104 01:37:28.009567   26544 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0104 01:37:28.010254   26544 ubuntu.go:169] provisioning hostname "minikube"
I0104 01:37:28.051888   26544 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 01:37:28.317858   26544 main.go:141] libmachine: Using SSH client type: native
I0104 01:37:28.318455   26544 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10fa1c0] 0x10fcda0 <nil>  [] 0s} 127.0.0.1 51321 <nil> <nil>}
I0104 01:37:28.318455   26544 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0104 01:37:28.550982   26544 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0104 01:37:28.592453   26544 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 01:37:28.898404   26544 main.go:141] libmachine: Using SSH client type: native
I0104 01:37:28.899046   26544 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10fa1c0] 0x10fcda0 <nil>  [] 0s} 127.0.0.1 51321 <nil> <nil>}
I0104 01:37:28.899269   26544 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0104 01:37:29.072833   26544 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0104 01:37:29.074342   26544 ubuntu.go:175] set auth options {CertDir:C:\Users\Payal Dhapodkar\.minikube CaCertPath:C:\Users\Payal Dhapodkar\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Payal Dhapodkar\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Payal Dhapodkar\.minikube\machines\server.pem ServerKeyPath:C:\Users\Payal Dhapodkar\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Payal Dhapodkar\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Payal Dhapodkar\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Payal Dhapodkar\.minikube}
I0104 01:37:29.074342   26544 ubuntu.go:177] setting up certificates
I0104 01:37:29.074342   26544 provision.go:84] configureAuth start
I0104 01:37:29.109906   26544 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0104 01:37:29.382023   26544 provision.go:143] copyHostCerts
I0104 01:37:29.383868   26544 exec_runner.go:144] found C:\Users\Payal Dhapodkar\.minikube/ca.pem, removing ...
I0104 01:37:29.383868   26544 exec_runner.go:203] rm: C:\Users\Payal Dhapodkar\.minikube\ca.pem
I0104 01:37:29.384874   26544 exec_runner.go:151] cp: C:\Users\Payal Dhapodkar\.minikube\certs\ca.pem --> C:\Users\Payal Dhapodkar\.minikube/ca.pem (1103 bytes)
I0104 01:37:29.387369   26544 exec_runner.go:144] found C:\Users\Payal Dhapodkar\.minikube/cert.pem, removing ...
I0104 01:37:29.387369   26544 exec_runner.go:203] rm: C:\Users\Payal Dhapodkar\.minikube\cert.pem
I0104 01:37:29.388436   26544 exec_runner.go:151] cp: C:\Users\Payal Dhapodkar\.minikube\certs\cert.pem --> C:\Users\Payal Dhapodkar\.minikube/cert.pem (1147 bytes)
I0104 01:37:29.393416   26544 exec_runner.go:144] found C:\Users\Payal Dhapodkar\.minikube/key.pem, removing ...
I0104 01:37:29.393884   26544 exec_runner.go:203] rm: C:\Users\Payal Dhapodkar\.minikube\key.pem
I0104 01:37:29.394373   26544 exec_runner.go:151] cp: C:\Users\Payal Dhapodkar\.minikube\certs\key.pem --> C:\Users\Payal Dhapodkar\.minikube/key.pem (1679 bytes)
I0104 01:37:29.397371   26544 provision.go:117] generating server cert: C:\Users\Payal Dhapodkar\.minikube\machines\server.pem ca-key=C:\Users\Payal Dhapodkar\.minikube\certs\ca.pem private-key=C:\Users\Payal Dhapodkar\.minikube\certs\ca-key.pem org=Payal Dhapodkar.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0104 01:37:29.473983   26544 provision.go:177] copyRemoteCerts
I0104 01:37:29.506918   26544 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0104 01:37:29.525842   26544 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 01:37:29.813960   26544 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51321 SSHKeyPath:C:\Users\Payal Dhapodkar\.minikube\machines\minikube\id_rsa Username:docker}
I0104 01:37:29.934909   26544 ssh_runner.go:362] scp C:\Users\Payal Dhapodkar\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1103 bytes)
I0104 01:37:30.004702   26544 ssh_runner.go:362] scp C:\Users\Payal Dhapodkar\.minikube\machines\server.pem --> /etc/docker/server.pem (1204 bytes)
I0104 01:37:30.044672   26544 ssh_runner.go:362] scp C:\Users\Payal Dhapodkar\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0104 01:37:30.079437   26544 provision.go:87] duration metric: took 1.0035329s to configureAuth
I0104 01:37:30.079437   26544 ubuntu.go:193] setting minikube options for container-runtime
I0104 01:37:30.080492   26544 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0104 01:37:30.097335   26544 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 01:37:30.360637   26544 main.go:141] libmachine: Using SSH client type: native
I0104 01:37:30.361137   26544 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10fa1c0] 0x10fcda0 <nil>  [] 0s} 127.0.0.1 51321 <nil> <nil>}
I0104 01:37:30.361137   26544 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0104 01:37:30.514758   26544 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0104 01:37:30.515257   26544 ubuntu.go:71] root file system type: overlay
I0104 01:37:30.515257   26544 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0104 01:37:30.540489   26544 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 01:37:30.781178   26544 main.go:141] libmachine: Using SSH client type: native
I0104 01:37:30.784185   26544 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10fa1c0] 0x10fcda0 <nil>  [] 0s} 127.0.0.1 51321 <nil> <nil>}
I0104 01:37:30.784683   26544 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0104 01:37:30.983605   26544 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0104 01:37:31.014408   26544 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 01:37:31.237407   26544 main.go:141] libmachine: Using SSH client type: native
I0104 01:37:31.238069   26544 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10fa1c0] 0x10fcda0 <nil>  [] 0s} 127.0.0.1 51321 <nil> <nil>}
I0104 01:37:31.238069   26544 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0104 01:37:32.749237   26544 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-11 10:51:59.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-01-03 20:07:30.961549680 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0104 01:37:32.749237   26544 machine.go:97] duration metric: took 5.1486628s to provisionDockerMachine
I0104 01:37:32.749237   26544 client.go:171] duration metric: took 35.9621699s to LocalClient.Create
I0104 01:37:32.749437   26544 start.go:167] duration metric: took 35.9623699s to libmachine.API.Create "minikube"
I0104 01:37:32.751476   26544 start.go:293] postStartSetup for "minikube" (driver="docker")
I0104 01:37:32.751476   26544 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0104 01:37:32.791362   26544 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0104 01:37:32.815366   26544 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 01:37:33.059015   26544 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51321 SSHKeyPath:C:\Users\Payal Dhapodkar\.minikube\machines\minikube\id_rsa Username:docker}
I0104 01:37:33.197374   26544 ssh_runner.go:195] Run: cat /etc/os-release
I0104 01:37:33.207777   26544 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0104 01:37:33.207777   26544 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0104 01:37:33.207777   26544 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0104 01:37:33.207777   26544 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0104 01:37:33.208642   26544 filesync.go:126] Scanning C:\Users\Payal Dhapodkar\.minikube\addons for local assets ...
I0104 01:37:33.209732   26544 filesync.go:126] Scanning C:\Users\Payal Dhapodkar\.minikube\files for local assets ...
I0104 01:37:33.210070   26544 start.go:296] duration metric: took 458.5936ms for postStartSetup
I0104 01:37:33.240073   26544 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0104 01:37:33.517289   26544 profile.go:143] Saving config to C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\config.json ...
I0104 01:37:33.543198   26544 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0104 01:37:33.599551   26544 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 01:37:33.808377   26544 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51321 SSHKeyPath:C:\Users\Payal Dhapodkar\.minikube\machines\minikube\id_rsa Username:docker}
I0104 01:37:33.951587   26544 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0104 01:37:33.969836   26544 start.go:128] duration metric: took 37.1891013s to createHost
I0104 01:37:33.969836   26544 start.go:83] releasing machines lock for "minikube", held for 37.1917906s
I0104 01:37:34.001615   26544 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0104 01:37:34.324538   26544 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0104 01:37:34.334454   26544 ssh_runner.go:195] Run: cat /version.json
I0104 01:37:34.354185   26544 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 01:37:34.360192   26544 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 01:37:34.573657   26544 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51321 SSHKeyPath:C:\Users\Payal Dhapodkar\.minikube\machines\minikube\id_rsa Username:docker}
I0104 01:37:34.593270   26544 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51321 SSHKeyPath:C:\Users\Payal Dhapodkar\.minikube\machines\minikube\id_rsa Username:docker}
I0104 01:37:34.704424   26544 ssh_runner.go:195] Run: systemctl --version
I0104 01:37:35.012719   26544 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0104 01:37:35.047421   26544 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0104 01:37:35.074093   26544 start.go:438] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0104 01:37:35.095290   26544 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0104 01:37:35.143959   26544 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0104 01:37:35.143959   26544 start.go:494] detecting cgroup driver to use...
I0104 01:37:35.143959   26544 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0104 01:37:35.145930   26544 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0104 01:37:35.203282   26544 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0104 01:37:35.231870   26544 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0104 01:37:35.247571   26544 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0104 01:37:35.257232   26544 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0104 01:37:35.282842   26544 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0104 01:37:35.304439   26544 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0104 01:37:35.324077   26544 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0104 01:37:35.349187   26544 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0104 01:37:35.378340   26544 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0104 01:37:35.409766   26544 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0104 01:37:35.432591   26544 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0104 01:37:35.464436   26544 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0104 01:37:35.508793   26544 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0104 01:37:35.549012   26544 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 01:37:35.689927   26544 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0104 01:37:35.866565   26544 start.go:494] detecting cgroup driver to use...
I0104 01:37:35.866565   26544 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0104 01:37:35.887065   26544 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0104 01:37:35.909026   26544 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0104 01:37:35.931968   26544 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0104 01:37:35.948937   26544 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0104 01:37:35.973433   26544 ssh_runner.go:195] Run: which cri-dockerd
I0104 01:37:35.998934   26544 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0104 01:37:36.012375   26544 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0104 01:37:36.060628   26544 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0104 01:37:36.191090   26544 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0104 01:37:36.299858   26544 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0104 01:37:36.299858   26544 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0104 01:37:36.344350   26544 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 01:37:36.511083   26544 ssh_runner.go:195] Run: sudo systemctl restart docker
I0104 01:37:37.138184   26544 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0104 01:37:37.193056   26544 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0104 01:37:37.256099   26544 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0104 01:37:37.445445   26544 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0104 01:37:37.605831   26544 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 01:37:37.725552   26544 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0104 01:37:37.777304   26544 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0104 01:37:37.811117   26544 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 01:37:37.946163   26544 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0104 01:37:38.339826   26544 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0104 01:37:38.347785   26544 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0104 01:37:38.353698   26544 start.go:562] Will wait 60s for crictl version
I0104 01:37:38.360221   26544 ssh_runner.go:195] Run: which crictl
I0104 01:37:38.384647   26544 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0104 01:37:38.624458   26544 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0104 01:37:38.641677   26544 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0104 01:37:38.796326   26544 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0104 01:37:38.826573   26544 out.go:204] 🐳  Preparing Kubernetes v1.30.0 on Docker 26.0.1 ...
I0104 01:37:38.842572   26544 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0104 01:37:39.180713   26544 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0104 01:37:39.187715   26544 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0104 01:37:39.194187   26544 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0104 01:37:39.220741   26544 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0104 01:37:39.482032   26544 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Payal Dhapodkar:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0104 01:37:39.482396   26544 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0104 01:37:39.503891   26544 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0104 01:37:39.526794   26544 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0104 01:37:39.526794   26544 docker.go:615] Images already preloaded, skipping extraction
I0104 01:37:39.538427   26544 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0104 01:37:39.559487   26544 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0104 01:37:39.561617   26544 cache_images.go:84] Images are preloaded, skipping loading
I0104 01:37:39.562342   26544 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0104 01:37:39.568300   26544 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0104 01:37:39.581601   26544 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0104 01:37:39.877153   26544 cni.go:84] Creating CNI manager for ""
I0104 01:37:39.877153   26544 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0104 01:37:39.877153   26544 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0104 01:37:39.877153   26544 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0104 01:37:39.877153   26544 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0104 01:37:39.893264   26544 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0104 01:37:39.906807   26544 binaries.go:44] Found k8s binaries, skipping transfer
I0104 01:37:39.934481   26544 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0104 01:37:39.949373   26544 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0104 01:37:39.976406   26544 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0104 01:37:39.999030   26544 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0104 01:37:40.038679   26544 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0104 01:37:40.047468   26544 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0104 01:37:40.086729   26544 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 01:37:40.211711   26544 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0104 01:37:40.232990   26544 certs.go:68] Setting up C:\Users\Payal Dhapodkar\.minikube\profiles\minikube for IP: 192.168.49.2
I0104 01:37:40.232990   26544 certs.go:194] generating shared ca certs ...
I0104 01:37:40.232990   26544 certs.go:226] acquiring lock for ca certs: {Name:mk21fe25407a09909d8899f0c70ad09ac299574b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 01:37:40.234464   26544 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\Payal Dhapodkar\.minikube\ca.key
I0104 01:37:40.236492   26544 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\Payal Dhapodkar\.minikube\proxy-client-ca.key
I0104 01:37:40.237267   26544 certs.go:256] generating profile certs ...
I0104 01:37:40.237964   26544 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\client.key
I0104 01:37:40.238538   26544 crypto.go:68] Generating cert C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\client.crt with IP's: []
I0104 01:37:40.393575   26544 crypto.go:156] Writing cert to C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\client.crt ...
I0104 01:37:40.393575   26544 lock.go:35] WriteFile acquiring C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\client.crt: {Name:mkf534559ca61459f8dc487186af7a3f5bb24371 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 01:37:40.397593   26544 crypto.go:164] Writing key to C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\client.key ...
I0104 01:37:40.397593   26544 lock.go:35] WriteFile acquiring C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\client.key: {Name:mk924b318b23ae34704655e7c3e0e4f3a5ae57f0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 01:37:40.400586   26544 certs.go:363] generating signed profile cert for "minikube": C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0104 01:37:40.400586   26544 crypto.go:68] Generating cert C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0104 01:37:40.678880   26544 crypto.go:156] Writing cert to C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0104 01:37:40.678880   26544 lock.go:35] WriteFile acquiring C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mk0fc7439ed71fff41c4793cde44ca81d2a6707c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 01:37:40.699775   26544 crypto.go:164] Writing key to C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0104 01:37:40.699775   26544 lock.go:35] WriteFile acquiring C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mkfa2dcaca9940f1b9387c51ebe9d55533379c03 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 01:37:40.704650   26544 certs.go:381] copying C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\apiserver.crt
I0104 01:37:40.713909   26544 certs.go:385] copying C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\apiserver.key
I0104 01:37:40.717203   26544 certs.go:363] generating signed profile cert for "aggregator": C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\proxy-client.key
I0104 01:37:40.717203   26544 crypto.go:68] Generating cert C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0104 01:37:41.051641   26544 crypto.go:156] Writing cert to C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\proxy-client.crt ...
I0104 01:37:41.051641   26544 lock.go:35] WriteFile acquiring C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\proxy-client.crt: {Name:mk90708fd24d1d312c9ffbcee4039b3b18391ea3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 01:37:41.054206   26544 crypto.go:164] Writing key to C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\proxy-client.key ...
I0104 01:37:41.054206   26544 lock.go:35] WriteFile acquiring C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\proxy-client.key: {Name:mk3005c86e139699033900112434fa81556f91cd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 01:37:41.059647   26544 certs.go:484] found cert: C:\Users\Payal Dhapodkar\.minikube\certs\ca-key.pem (1675 bytes)
I0104 01:37:41.060658   26544 certs.go:484] found cert: C:\Users\Payal Dhapodkar\.minikube\certs\ca.pem (1103 bytes)
I0104 01:37:41.060658   26544 certs.go:484] found cert: C:\Users\Payal Dhapodkar\.minikube\certs\cert.pem (1147 bytes)
I0104 01:37:41.061755   26544 certs.go:484] found cert: C:\Users\Payal Dhapodkar\.minikube\certs\key.pem (1679 bytes)
I0104 01:37:41.070660   26544 ssh_runner.go:362] scp C:\Users\Payal Dhapodkar\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0104 01:37:41.106754   26544 ssh_runner.go:362] scp C:\Users\Payal Dhapodkar\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0104 01:37:41.151400   26544 ssh_runner.go:362] scp C:\Users\Payal Dhapodkar\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0104 01:37:41.208747   26544 ssh_runner.go:362] scp C:\Users\Payal Dhapodkar\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0104 01:37:41.259836   26544 ssh_runner.go:362] scp C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0104 01:37:41.303211   26544 ssh_runner.go:362] scp C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0104 01:37:41.350592   26544 ssh_runner.go:362] scp C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0104 01:37:41.397895   26544 ssh_runner.go:362] scp C:\Users\Payal Dhapodkar\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0104 01:37:41.448142   26544 ssh_runner.go:362] scp C:\Users\Payal Dhapodkar\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0104 01:37:41.487901   26544 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0104 01:37:41.547363   26544 ssh_runner.go:195] Run: openssl version
I0104 01:37:41.584969   26544 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0104 01:37:41.605662   26544 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0104 01:37:41.611954   26544 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Dec 12 13:04 /usr/share/ca-certificates/minikubeCA.pem
I0104 01:37:41.619633   26544 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0104 01:37:41.654491   26544 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0104 01:37:41.680737   26544 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0104 01:37:41.687253   26544 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0104 01:37:41.687732   26544 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Payal Dhapodkar:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0104 01:37:41.699231   26544 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0104 01:37:41.747832   26544 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0104 01:37:41.779870   26544 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0104 01:37:41.798454   26544 kubeadm.go:213] ignoring SystemVerification for kubeadm because of docker driver
I0104 01:37:41.819460   26544 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0104 01:37:41.833012   26544 kubeadm.go:154] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0104 01:37:41.833012   26544 kubeadm.go:156] found existing configuration files:

I0104 01:37:41.851462   26544 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0104 01:37:41.867267   26544 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0104 01:37:41.895499   26544 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0104 01:37:41.945421   26544 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0104 01:37:41.966328   26544 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0104 01:37:41.996844   26544 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0104 01:37:42.046817   26544 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0104 01:37:42.067769   26544 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0104 01:37:42.098877   26544 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0104 01:37:42.150269   26544 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0104 01:37:42.163452   26544 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0104 01:37:42.183074   26544 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0104 01:37:42.197730   26544 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0104 01:37:42.341784   26544 kubeadm.go:309] 	[WARNING Swap]: swap is supported for cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default
I0104 01:37:42.481427   26544 kubeadm.go:309] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0104 01:37:59.344431   26544 kubeadm.go:309] [init] Using Kubernetes version: v1.30.0
I0104 01:37:59.344968   26544 kubeadm.go:309] [preflight] Running pre-flight checks
I0104 01:37:59.345361   26544 kubeadm.go:309] [preflight] Pulling images required for setting up a Kubernetes cluster
I0104 01:37:59.345684   26544 kubeadm.go:309] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0104 01:37:59.346070   26544 kubeadm.go:309] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0104 01:37:59.346425   26544 kubeadm.go:309] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0104 01:37:59.349873   26544 out.go:204]     ▪ Generating certificates and keys ...
I0104 01:37:59.351865   26544 kubeadm.go:309] [certs] Using existing ca certificate authority
I0104 01:37:59.352363   26544 kubeadm.go:309] [certs] Using existing apiserver certificate and key on disk
I0104 01:37:59.352363   26544 kubeadm.go:309] [certs] Generating "apiserver-kubelet-client" certificate and key
I0104 01:37:59.352858   26544 kubeadm.go:309] [certs] Generating "front-proxy-ca" certificate and key
I0104 01:37:59.352858   26544 kubeadm.go:309] [certs] Generating "front-proxy-client" certificate and key
I0104 01:37:59.353361   26544 kubeadm.go:309] [certs] Generating "etcd/ca" certificate and key
I0104 01:37:59.353361   26544 kubeadm.go:309] [certs] Generating "etcd/server" certificate and key
I0104 01:37:59.353361   26544 kubeadm.go:309] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0104 01:37:59.353862   26544 kubeadm.go:309] [certs] Generating "etcd/peer" certificate and key
I0104 01:37:59.354362   26544 kubeadm.go:309] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0104 01:37:59.354362   26544 kubeadm.go:309] [certs] Generating "etcd/healthcheck-client" certificate and key
I0104 01:37:59.354362   26544 kubeadm.go:309] [certs] Generating "apiserver-etcd-client" certificate and key
I0104 01:37:59.354362   26544 kubeadm.go:309] [certs] Generating "sa" key and public key
I0104 01:37:59.354864   26544 kubeadm.go:309] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0104 01:37:59.354864   26544 kubeadm.go:309] [kubeconfig] Writing "admin.conf" kubeconfig file
I0104 01:37:59.354864   26544 kubeadm.go:309] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0104 01:37:59.354864   26544 kubeadm.go:309] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0104 01:37:59.355358   26544 kubeadm.go:309] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0104 01:37:59.355358   26544 kubeadm.go:309] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0104 01:37:59.355358   26544 kubeadm.go:309] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0104 01:37:59.355862   26544 kubeadm.go:309] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0104 01:37:59.358454   26544 out.go:204]     ▪ Booting up control plane ...
I0104 01:37:59.359860   26544 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0104 01:37:59.360358   26544 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0104 01:37:59.360358   26544 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0104 01:37:59.360862   26544 kubeadm.go:309] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0104 01:37:59.360862   26544 kubeadm.go:309] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0104 01:37:59.360862   26544 kubeadm.go:309] [kubelet-start] Starting the kubelet
I0104 01:37:59.361363   26544 kubeadm.go:309] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0104 01:37:59.361363   26544 kubeadm.go:309] [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
I0104 01:37:59.361859   26544 kubeadm.go:309] [kubelet-check] The kubelet is healthy after 502.075077ms
I0104 01:37:59.361859   26544 kubeadm.go:309] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0104 01:37:59.361859   26544 kubeadm.go:309] [api-check] The API server is healthy after 11.502324329s
I0104 01:37:59.361859   26544 kubeadm.go:309] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0104 01:37:59.361859   26544 kubeadm.go:309] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0104 01:37:59.361859   26544 kubeadm.go:309] [upload-certs] Skipping phase. Please see --upload-certs
I0104 01:37:59.362358   26544 kubeadm.go:309] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0104 01:37:59.362358   26544 kubeadm.go:309] [bootstrap-token] Using token: uboh0d.fodn8huycef0utoh
I0104 01:37:59.366808   26544 out.go:204]     ▪ Configuring RBAC rules ...
I0104 01:37:59.367349   26544 kubeadm.go:309] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0104 01:37:59.367349   26544 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0104 01:37:59.367880   26544 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0104 01:37:59.367880   26544 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0104 01:37:59.367880   26544 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0104 01:37:59.368404   26544 kubeadm.go:309] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0104 01:37:59.368404   26544 kubeadm.go:309] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0104 01:37:59.368404   26544 kubeadm.go:309] [addons] Applied essential addon: CoreDNS
I0104 01:37:59.368404   26544 kubeadm.go:309] [addons] Applied essential addon: kube-proxy
I0104 01:37:59.368404   26544 kubeadm.go:309] 
I0104 01:37:59.368404   26544 kubeadm.go:309] Your Kubernetes control-plane has initialized successfully!
I0104 01:37:59.368404   26544 kubeadm.go:309] 
I0104 01:37:59.368404   26544 kubeadm.go:309] To start using your cluster, you need to run the following as a regular user:
I0104 01:37:59.368404   26544 kubeadm.go:309] 
I0104 01:37:59.368404   26544 kubeadm.go:309]   mkdir -p $HOME/.kube
I0104 01:37:59.368404   26544 kubeadm.go:309]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0104 01:37:59.368404   26544 kubeadm.go:309]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0104 01:37:59.368404   26544 kubeadm.go:309] 
I0104 01:37:59.369395   26544 kubeadm.go:309] Alternatively, if you are the root user, you can run:
I0104 01:37:59.369395   26544 kubeadm.go:309] 
I0104 01:37:59.369395   26544 kubeadm.go:309]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0104 01:37:59.369395   26544 kubeadm.go:309] 
I0104 01:37:59.369395   26544 kubeadm.go:309] You should now deploy a pod network to the cluster.
I0104 01:37:59.369395   26544 kubeadm.go:309] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0104 01:37:59.369395   26544 kubeadm.go:309]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0104 01:37:59.369395   26544 kubeadm.go:309] 
I0104 01:37:59.369395   26544 kubeadm.go:309] You can now join any number of control-plane nodes by copying certificate authorities
I0104 01:37:59.369395   26544 kubeadm.go:309] and service account keys on each node and then running the following as root:
I0104 01:37:59.369395   26544 kubeadm.go:309] 
I0104 01:37:59.369395   26544 kubeadm.go:309]   kubeadm join control-plane.minikube.internal:8443 --token uboh0d.fodn8huycef0utoh \
I0104 01:37:59.369395   26544 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:c857586727d297aabe161e29151f9a3ef30b66d332dec0e7b60dd3691b446dee \
I0104 01:37:59.369395   26544 kubeadm.go:309] 	--control-plane 
I0104 01:37:59.369395   26544 kubeadm.go:309] 
I0104 01:37:59.370395   26544 kubeadm.go:309] Then you can join any number of worker nodes by running the following on each as root:
I0104 01:37:59.370395   26544 kubeadm.go:309] 
I0104 01:37:59.370395   26544 kubeadm.go:309] kubeadm join control-plane.minikube.internal:8443 --token uboh0d.fodn8huycef0utoh \
I0104 01:37:59.370395   26544 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:c857586727d297aabe161e29151f9a3ef30b66d332dec0e7b60dd3691b446dee 
I0104 01:37:59.370395   26544 cni.go:84] Creating CNI manager for ""
I0104 01:37:59.370395   26544 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0104 01:37:59.374955   26544 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0104 01:37:59.404119   26544 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0104 01:37:59.423548   26544 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0104 01:37:59.445891   26544 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0104 01:37:59.459324   26544 ops.go:34] apiserver oom_adj: -16
I0104 01:37:59.470371   26544 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_01_04T01_37_59_0700 minikube.k8s.io/version=v1.33.0 minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0104 01:37:59.470371   26544 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0104 01:37:59.553463   26544 kubeadm.go:1107] duration metric: took 106.5892ms to wait for elevateKubeSystemPrivileges
W0104 01:37:59.580748   26544 kubeadm.go:286] apiserver tunnel failed: apiserver port not set
I0104 01:37:59.580748   26544 kubeadm.go:393] duration metric: took 17.8930161s to StartCluster
I0104 01:37:59.581335   26544 settings.go:142] acquiring lock: {Name:mkc8c243f5de26801020004995fd580962e5c5ca Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 01:37:59.581860   26544 settings.go:150] Updating kubeconfig:  C:\Users\Payal Dhapodkar\.kube\config
I0104 01:37:59.585088   26544 lock.go:35] WriteFile acquiring C:\Users\Payal Dhapodkar\.kube\config: {Name:mk8dd808c9caade54b57c779508998a972a2b660 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 01:37:59.587591   26544 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0104 01:37:59.587591   26544 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0104 01:37:59.587591   26544 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0104 01:37:59.590106   26544 out.go:177] 🔎  Verifying Kubernetes components...
I0104 01:37:59.588088   26544 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0104 01:37:59.590106   26544 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0104 01:37:59.590106   26544 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0104 01:37:59.597107   26544 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0104 01:37:59.597107   26544 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0104 01:37:59.599588   26544 host.go:66] Checking if "minikube" exists ...
I0104 01:37:59.672646   26544 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0104 01:37:59.683217   26544 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 01:37:59.703273   26544 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0104 01:37:59.711875   26544 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0104 01:37:59.901001   26544 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0104 01:37:59.913495   26544 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0104 01:37:59.913495   26544 host.go:66] Checking if "minikube" exists ...
I0104 01:37:59.931257   26544 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0104 01:37:59.937040   26544 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0104 01:37:59.937040   26544 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0104 01:37:59.951651   26544 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0104 01:37:59.960797   26544 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 01:38:00.050225   26544 start.go:946] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0104 01:38:00.081915   26544 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0104 01:38:00.205149   26544 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0104 01:38:00.205149   26544 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0104 01:38:00.219800   26544 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51321 SSHKeyPath:C:\Users\Payal Dhapodkar\.minikube\machines\minikube\id_rsa Username:docker}
I0104 01:38:00.219800   26544 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 01:38:00.315534   26544 api_server.go:52] waiting for apiserver process to appear ...
I0104 01:38:00.340768   26544 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0104 01:38:00.351271   26544 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0104 01:38:00.354437   26544 api_server.go:72] duration metric: took 766.8465ms to wait for apiserver process to appear ...
I0104 01:38:00.354437   26544 api_server.go:88] waiting for apiserver healthz status ...
I0104 01:38:00.354788   26544 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51325/healthz ...
I0104 01:38:00.363763   26544 api_server.go:279] https://127.0.0.1:51325/healthz returned 200:
ok
I0104 01:38:00.366762   26544 api_server.go:141] control plane version: v1.30.0
I0104 01:38:00.366762   26544 api_server.go:131] duration metric: took 12.325ms to wait for apiserver health ...
I0104 01:38:00.366762   26544 system_pods.go:43] waiting for kube-system pods to appear ...
I0104 01:38:00.379262   26544 system_pods.go:59] 4 kube-system pods found
I0104 01:38:00.379262   26544 system_pods.go:61] "etcd-minikube" [7f21b98d-9787-49df-a6fd-0b2864fe69c7] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0104 01:38:00.379262   26544 system_pods.go:61] "kube-apiserver-minikube" [1654fecc-e888-4ee7-b303-b50308cf5712] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0104 01:38:00.379262   26544 system_pods.go:61] "kube-controller-manager-minikube" [314be776-1189-483b-8f5b-3430835b046d] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0104 01:38:00.379262   26544 system_pods.go:61] "kube-scheduler-minikube" [041f11ec-04de-46c6-b693-dd5ba758b12a] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0104 01:38:00.379262   26544 system_pods.go:74] duration metric: took 12.5001ms to wait for pod list to return data ...
I0104 01:38:00.379262   26544 kubeadm.go:576] duration metric: took 791.6716ms to wait for: map[apiserver:true system_pods:true]
I0104 01:38:00.379262   26544 node_conditions.go:102] verifying NodePressure condition ...
I0104 01:38:00.385304   26544 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0104 01:38:00.385304   26544 node_conditions.go:123] node cpu capacity is 8
I0104 01:38:00.385304   26544 node_conditions.go:105] duration metric: took 6.0417ms to run NodePressure ...
I0104 01:38:00.385304   26544 start.go:240] waiting for startup goroutines ...
I0104 01:38:00.421372   26544 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51321 SSHKeyPath:C:\Users\Payal Dhapodkar\.minikube\machines\minikube\id_rsa Username:docker}
I0104 01:38:00.557230   26544 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0104 01:38:00.563965   26544 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0104 01:38:00.707245   26544 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0104 01:38:00.710507   26544 addons.go:505] duration metric: took 1.1234198s for enable addons: enabled=[storage-provisioner default-storageclass]
I0104 01:38:00.710507   26544 start.go:245] waiting for cluster config update ...
I0104 01:38:00.710507   26544 start.go:254] writing updated cluster config ...
I0104 01:38:00.721857   26544 ssh_runner.go:195] Run: rm -f paused
I0104 01:38:00.888891   26544 start.go:600] kubectl: 1.29.2, cluster: 1.30.0 (minor skew: 1)
I0104 01:38:00.891563   26544 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jan 03 20:07:36 minikube dockerd[1221]: time="2025-01-03T20:07:36.690447865Z" level=info msg="Loading containers: start."
Jan 03 20:07:36 minikube dockerd[1221]: time="2025-01-03T20:07:36.836492683Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Jan 03 20:07:36 minikube dockerd[1221]: time="2025-01-03T20:07:36.944113939Z" level=info msg="Loading containers: done."
Jan 03 20:07:36 minikube dockerd[1221]: time="2025-01-03T20:07:36.983271424Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Jan 03 20:07:36 minikube dockerd[1221]: time="2025-01-03T20:07:36.983345207Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Jan 03 20:07:36 minikube dockerd[1221]: time="2025-01-03T20:07:36.983359269Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Jan 03 20:07:36 minikube dockerd[1221]: time="2025-01-03T20:07:36.983372042Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Jan 03 20:07:36 minikube dockerd[1221]: time="2025-01-03T20:07:36.983417543Z" level=info msg="Docker daemon" commit=60b9add containerd-snapshotter=false storage-driver=overlay2 version=26.0.1
Jan 03 20:07:36 minikube dockerd[1221]: time="2025-01-03T20:07:36.983520476Z" level=info msg="Daemon has completed initialization"
Jan 03 20:07:37 minikube dockerd[1221]: time="2025-01-03T20:07:37.096934106Z" level=info msg="API listen on /var/run/docker.sock"
Jan 03 20:07:37 minikube dockerd[1221]: time="2025-01-03T20:07:37.096982911Z" level=info msg="API listen on [::]:2376"
Jan 03 20:07:37 minikube systemd[1]: Started Docker Application Container Engine.
Jan 03 20:07:37 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jan 03 20:07:38 minikube cri-dockerd[1446]: time="2025-01-03T20:07:38Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jan 03 20:07:38 minikube cri-dockerd[1446]: time="2025-01-03T20:07:38Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jan 03 20:07:38 minikube cri-dockerd[1446]: time="2025-01-03T20:07:38Z" level=info msg="Start docker client with request timeout 0s"
Jan 03 20:07:38 minikube cri-dockerd[1446]: time="2025-01-03T20:07:38Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jan 03 20:07:38 minikube cri-dockerd[1446]: time="2025-01-03T20:07:38Z" level=info msg="Loaded network plugin cni"
Jan 03 20:07:38 minikube cri-dockerd[1446]: time="2025-01-03T20:07:38Z" level=info msg="Docker cri networking managed by network plugin cni"
Jan 03 20:07:38 minikube cri-dockerd[1446]: time="2025-01-03T20:07:38Z" level=info msg="Setting cgroupDriver cgroupfs"
Jan 03 20:07:38 minikube cri-dockerd[1446]: time="2025-01-03T20:07:38Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jan 03 20:07:38 minikube cri-dockerd[1446]: time="2025-01-03T20:07:38Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jan 03 20:07:38 minikube cri-dockerd[1446]: time="2025-01-03T20:07:38Z" level=info msg="Start cri-dockerd grpc backend"
Jan 03 20:07:38 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jan 03 20:07:47 minikube cri-dockerd[1446]: time="2025-01-03T20:07:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b20f7c994ebe3a8c24e9ea3f436cf35d30e9702f76d771d722d67817c83c6b3d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 03 20:07:47 minikube cri-dockerd[1446]: time="2025-01-03T20:07:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9b85345f5830846fed681cbc22d0cafee7922c83b7837adb328e1fcd2f11b4c4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 03 20:07:47 minikube cri-dockerd[1446]: time="2025-01-03T20:07:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8d838027476aa8f3049d021dfc615c02f076e873edddcd817e489e74e5737d3d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 03 20:07:47 minikube cri-dockerd[1446]: time="2025-01-03T20:07:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/733f8deefae13a2bab521368c08bbb65f0a3fc505101afcb8175503cb8414cc8/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 03 20:08:13 minikube cri-dockerd[1446]: time="2025-01-03T20:08:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9437e63f4aa8905c0d81937d9310d8725ec139cd9a847419c8ee3e57eb20c3ac/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 03 20:08:13 minikube dockerd[1221]: time="2025-01-03T20:08:13.513921462Z" level=info msg="ignoring event" container=25d6d1f8362de2ecddd8ca93546bd9d70f5741d6d7f06c39830b9b4579c9bd6c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 03 20:08:14 minikube cri-dockerd[1446]: time="2025-01-03T20:08:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7b60d0c73498b5b216161ba44b7eae3e3b6987d0590668fdedd4ff0de9bea33d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 03 20:08:14 minikube cri-dockerd[1446]: time="2025-01-03T20:08:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/15f23bcf77ce768c432e6aa7d5841eeb70d9cc0b6c5d18cb998dc40c45338e62/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 03 20:08:14 minikube cri-dockerd[1446]: time="2025-01-03T20:08:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5823f69f5b36d6a649ea0266243c85b692875f0cd4d332e12545d27f070e3304/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 03 20:08:14 minikube cri-dockerd[1446]: time="2025-01-03T20:08:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a749f8f8fae09adf811839e33e4967319428d1e63aa54b50980082b4fe90e513/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 03 20:08:14 minikube dockerd[1221]: time="2025-01-03T20:08:14.944187585Z" level=info msg="ignoring event" container=05662dae44646ec9dad7ae299e3658feb0704f7a6df10284f51c18043f68761c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 03 20:08:16 minikube dockerd[1221]: time="2025-01-03T20:08:16.069647863Z" level=warning msg="Published ports are discarded when using host network mode"
Jan 03 20:08:16 minikube dockerd[1221]: time="2025-01-03T20:08:16.259653213Z" level=warning msg="Published ports are discarded when using host network mode"
Jan 03 20:08:16 minikube cri-dockerd[1446]: time="2025-01-03T20:08:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/45fcf4e272695c05cd84b039772991d1a7b56948c5d3badb491a0aaa9bfa1c1b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 03 20:08:16 minikube cri-dockerd[1446]: time="2025-01-03T20:08:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/789b4f238c13beaeed4462fe3c198b77dec3058a483f009ea36c1d97c6e342fe/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 03 20:08:17 minikube cri-dockerd[1446]: time="2025-01-03T20:08:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4b983e84cc51bd3a47a18ab984927cf0332e900ae3ea8a968c90c93d25e0f857/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 03 20:08:19 minikube cri-dockerd[1446]: time="2025-01-03T20:08:19Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jan 03 20:08:22 minikube cri-dockerd[1446]: time="2025-01-03T20:08:22Z" level=info msg="Stop pulling image payaldhapodkar/web-app-repo:latest: Status: Downloaded newer image for payaldhapodkar/web-app-repo:latest"
Jan 03 20:08:24 minikube cri-dockerd[1446]: time="2025-01-03T20:08:24Z" level=info msg="Stop pulling image payaldhapodkar/web-app-repo:latest: Status: Image is up to date for payaldhapodkar/web-app-repo:latest"
Jan 03 20:08:32 minikube cri-dockerd[1446]: time="2025-01-03T20:08:32Z" level=info msg="Stop pulling image quay.io/prometheus/node-exporter:v1.8.2: Status: Downloaded newer image for quay.io/prometheus/node-exporter:v1.8.2"
Jan 03 20:08:35 minikube cri-dockerd[1446]: time="2025-01-03T20:08:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2f9fa2ddf1f98f57198149f7ffc901cf5e3276190d141c6483cdb392454a934f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 03 20:08:37 minikube cri-dockerd[1446]: time="2025-01-03T20:08:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1fe69de400bb146439c0144b68f359484419eed1fe497bd72359464b34b2cbe2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 03 20:08:39 minikube cri-dockerd[1446]: time="2025-01-03T20:08:39Z" level=info msg="Stop pulling image quay.io/prometheus/pushgateway:v1.10.0: Status: Downloaded newer image for quay.io/prometheus/pushgateway:v1.10.0"
Jan 03 20:08:44 minikube cri-dockerd[1446]: time="2025-01-03T20:08:44Z" level=info msg="Stop pulling image registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0: Status: Downloaded newer image for registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0"
Jan 03 20:08:52 minikube cri-dockerd[1446]: time="2025-01-03T20:08:52Z" level=info msg="Stop pulling image quay.io/prometheus/alertmanager:v0.27.0: Status: Downloaded newer image for quay.io/prometheus/alertmanager:v0.27.0"
Jan 03 20:08:59 minikube cri-dockerd[1446]: time="2025-01-03T20:08:59Z" level=info msg="Stop pulling image quay.io/prometheus-operator/prometheus-config-reloader:v0.78.2: Status: Downloaded newer image for quay.io/prometheus-operator/prometheus-config-reloader:v0.78.2"
Jan 03 20:09:15 minikube cri-dockerd[1446]: time="2025-01-03T20:09:15Z" level=info msg="Pulling image quay.io/prometheus/prometheus:v3.1.0: 935b618cedfb: Extracting [============================================>      ]  48.46MB/54.61MB"
Jan 03 20:09:16 minikube cri-dockerd[1446]: time="2025-01-03T20:09:16Z" level=info msg="Stop pulling image quay.io/prometheus/prometheus:v3.1.0: Status: Downloaded newer image for quay.io/prometheus/prometheus:v3.1.0"
Jan 03 20:17:19 minikube dockerd[1221]: 2025/01/03 20:17:19 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jan 03 20:17:19 minikube dockerd[1221]: 2025/01/03 20:17:19 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jan 03 20:17:19 minikube dockerd[1221]: 2025/01/03 20:17:19 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jan 03 20:17:19 minikube dockerd[1221]: 2025/01/03 20:17:19 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jan 03 20:17:19 minikube dockerd[1221]: 2025/01/03 20:17:19 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jan 03 20:17:19 minikube dockerd[1221]: 2025/01/03 20:17:19 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jan 03 20:17:19 minikube dockerd[1221]: 2025/01/03 20:17:19 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jan 03 20:17:19 minikube dockerd[1221]: 2025/01/03 20:17:19 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)


==> container status <==
CONTAINER           IMAGE                                                                                                                            CREATED             STATE               NAME                                 ATTEMPT             POD ID              POD
6aa0dae275995       quay.io/prometheus/prometheus@sha256:6559acbd5d770b15bb3c954629ce190ac3cbbdb2b7f1c30f0385c4e05104e218                            9 minutes ago       Running             prometheus-server                    0                   1fe69de400bb1       prometheus-server-9976468d9-klkb2
3a88bda7dec0c       quay.io/prometheus-operator/prometheus-config-reloader@sha256:944b2c67345c2dd9fafc4cddbf389cb09f930f9e83c8d06e90147076223a9e56   10 minutes ago      Running             prometheus-server-configmap-reload   0                   1fe69de400bb1       prometheus-server-9976468d9-klkb2
a3c36a659f5fa       quay.io/prometheus/alertmanager@sha256:e13b6ed5cb929eeaee733479dce55e10eb3bc2e9c4586c705a4e8da41e5eacf5                          10 minutes ago      Running             alertmanager                         0                   2f9fa2ddf1f98       prometheus-alertmanager-0
f9f4a067560ae       registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:37d841299325c23b56e5951176ce8ef317d537447c0f1b2d2437dddbb1f51165    10 minutes ago      Running             kube-state-metrics                   0                   4b983e84cc51b       prometheus-kube-state-metrics-8ff4967cb-stqs9
4938d0c638567       quay.io/prometheus/pushgateway@sha256:7a4d0696a24ef4e8bad62bee5656855a0aff2f26416d8cb32009dc28d6263604                           10 minutes ago      Running             pushgateway                          0                   789b4f238c13b       prometheus-prometheus-pushgateway-7d7b6497cd-mnl6z
3674df34bea3c       quay.io/prometheus/node-exporter@sha256:4032c6d5bfd752342c3e631c2f1de93ba6b86c41db6b167b9a35372c139e7706                         10 minutes ago      Running             node-exporter                        0                   45fcf4e272695       prometheus-prometheus-node-exporter-lnj8b
2f84b013c4163       6e38f40d628db                                                                                                                    10 minutes ago      Running             storage-provisioner                  2                   9437e63f4aa89       storage-provisioner
f534a1e736b0e       payaldhapodkar/web-app-repo@sha256:b9b4f4313eb10a4f0a782ed4b4e84d8cd6587d7b179a31f244458b00748bb5be                              10 minutes ago      Running             web-app                              0                   5823f69f5b36d       web-app-deployment-78c4d59b57-cq6wq
79e6cfdbe2250       payaldhapodkar/web-app-repo@sha256:b9b4f4313eb10a4f0a782ed4b4e84d8cd6587d7b179a31f244458b00748bb5be                              10 minutes ago      Running             web-app                              0                   a749f8f8fae09       web-app-deployment-78c4d59b57-hbmmv
7a093ea234420       cbb01a7bd410d                                                                                                                    10 minutes ago      Running             coredns                              0                   15f23bcf77ce7       coredns-7db6d8ff4d-lxkcs
05662dae44646       6e38f40d628db                                                                                                                    10 minutes ago      Exited              storage-provisioner                  1                   9437e63f4aa89       storage-provisioner
df54198334153       a0bf559e280cf                                                                                                                    10 minutes ago      Running             kube-proxy                           0                   7b60d0c73498b       kube-proxy-726vc
68f35cc3b8435       c7aad43836fa5                                                                                                                    11 minutes ago      Running             kube-controller-manager              0                   733f8deefae13       kube-controller-manager-minikube
08503fcf66b0b       3861cfcd7c04c                                                                                                                    11 minutes ago      Running             etcd                                 0                   8d838027476aa       etcd-minikube
ccd3894d93a8f       259c8277fcbbc                                                                                                                    11 minutes ago      Running             kube-scheduler                       0                   9b85345f58308       kube-scheduler-minikube
a466d7953f8c6       c42f13656d0b2                                                                                                                    11 minutes ago      Running             kube-apiserver                       0                   b20f7c994ebe3       kube-apiserver-minikube


==> coredns [7a093ea23442] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:37205 - 47956 "HINFO IN 797473037649442062.5374098261718804985. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.136885009s
[INFO] 10.244.0.8:35734 - 30200 "A IN kubernetes.default.svc.default.svc.cluster.local. udp 77 false 1232" NXDOMAIN qr,aa,rd 159 0.000406864s
[INFO] 10.244.0.8:44130 - 27641 "AAAA IN kubernetes.default.svc.default.svc.cluster.local. udp 77 false 1232" NXDOMAIN qr,aa,rd 159 0.000514592s
[INFO] 10.244.0.8:33381 - 43752 "A IN kubernetes.default.svc.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000335475s
[INFO] 10.244.0.8:43563 - 3753 "AAAA IN kubernetes.default.svc.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000160659s
[INFO] 10.244.0.8:46630 - 29034 "AAAA IN kubernetes.default.svc.cluster.local. udp 65 false 1232" NOERROR qr,aa,rd 147 0.000248904s
[INFO] 10.244.0.8:33892 - 31162 "A IN kubernetes.default.svc.cluster.local. udp 65 false 1232" NOERROR qr,aa,rd 106 0.000317753s
[INFO] 10.244.0.8:45625 - 30332 "A IN prometheus-prometheus-pushgateway.default.svc.default.svc.cluster.local. udp 100 false 1232" NXDOMAIN qr,aa,rd 182 0.000344428s
[INFO] 10.244.0.8:50091 - 61762 "AAAA IN prometheus-prometheus-pushgateway.default.svc.default.svc.cluster.local. udp 100 false 1232" NXDOMAIN qr,aa,rd 182 0.000388619s
[INFO] 10.244.0.8:59856 - 24931 "A IN prometheus-prometheus-pushgateway.default.svc.svc.cluster.local. udp 92 false 1232" NXDOMAIN qr,aa,rd 174 0.000435882s
[INFO] 10.244.0.8:58719 - 28966 "AAAA IN prometheus-prometheus-pushgateway.default.svc.svc.cluster.local. udp 92 false 1232" NXDOMAIN qr,aa,rd 174 0.000521323s
[INFO] 10.244.0.8:39220 - 54293 "A IN prometheus-prometheus-pushgateway.default.svc.cluster.local. udp 88 false 1232" NOERROR qr,aa,rd 152 0.000280778s
[INFO] 10.244.0.8:44158 - 4608 "AAAA IN prometheus-prometheus-pushgateway.default.svc.cluster.local. udp 88 false 1232" NOERROR qr,aa,rd 170 0.000247207s
[INFO] 10.244.0.8:54811 - 37644 "A IN kubernetes.default.svc.default.svc.cluster.local. udp 77 false 1232" NXDOMAIN qr,aa,rd 159 0.0003556s
[INFO] 10.244.0.8:57061 - 10613 "AAAA IN kubernetes.default.svc.default.svc.cluster.local. udp 77 false 1232" NXDOMAIN qr,aa,rd 159 0.000396087s
[INFO] 10.244.0.8:51381 - 10037 "A IN kubernetes.default.svc.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000397357s
[INFO] 10.244.0.8:44927 - 20594 "AAAA IN kubernetes.default.svc.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000375957s
[INFO] 10.244.0.8:44842 - 54575 "A IN kubernetes.default.svc.cluster.local. udp 65 false 1232" NOERROR qr,aa,rd 106 0.000501794s
[INFO] 10.244.0.8:46189 - 23428 "AAAA IN kubernetes.default.svc.cluster.local. udp 65 false 1232" NOERROR qr,aa,rd 147 0.000447452s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_01_04T01_37_59_0700
                    minikube.k8s.io/version=v1.33.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 03 Jan 2025 20:07:52 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 03 Jan 2025 20:19:01 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 03 Jan 2025 20:14:37 +0000   Fri, 03 Jan 2025 20:07:49 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 03 Jan 2025 20:14:37 +0000   Fri, 03 Jan 2025 20:07:49 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 03 Jan 2025 20:14:37 +0000   Fri, 03 Jan 2025 20:07:49 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 03 Jan 2025 20:14:37 +0000   Fri, 03 Jan 2025 20:07:52 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8001608Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8001608Ki
  pods:               110
System Info:
  Machine ID:                 06e0bf89e3a74202afbce4fe224070f2
  System UUID:                06e0bf89e3a74202afbce4fe224070f2
  Boot ID:                    9601b5b9-9784-4087-8434-3eef7a87d490
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.0.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                  ------------  ----------  ---------------  -------------  ---
  default                     prometheus-alertmanager-0                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     prometheus-kube-state-metrics-8ff4967cb-stqs9         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     prometheus-prometheus-node-exporter-lnj8b             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     prometheus-prometheus-pushgateway-7d7b6497cd-mnl6z    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     prometheus-server-9976468d9-klkb2                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     web-app-deployment-78c4d59b57-cq6wq                   0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     web-app-deployment-78c4d59b57-hbmmv                   0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  kube-system                 coredns-7db6d8ff4d-lxkcs                              100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     10m
  kube-system                 etcd-minikube                                         100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         11m
  kube-system                 kube-apiserver-minikube                               250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11m
  kube-system                 kube-controller-manager-minikube                      200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11m
  kube-system                 kube-proxy-726vc                                      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  kube-system                 kube-scheduler-minikube                               100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11m
  kube-system                 storage-provisioner                                   0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 10m                kube-proxy       
  Normal  NodeHasSufficientMemory  11m (x8 over 11m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    11m (x8 over 11m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     11m (x7 over 11m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  11m                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 11m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  11m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    11m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     11m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  11m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           10m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000481] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000320] FS-Cache: O-cookie d=00000000aed99636{9P.session} n=000000001cf64707
[  +0.000496] FS-Cache: O-key=[10] '34323934393337343137'
[  +0.000431] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.000326] FS-Cache: N-cookie d=00000000aed99636{9P.session} n=000000007e520587
[  +0.000466] FS-Cache: N-key=[10] '34323934393337343137'
[  +0.001847] FS-Cache: Duplicate cookie detected
[  +0.000402] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000353] FS-Cache: O-cookie d=00000000aed99636{9P.session} n=000000001cf64707
[  +0.000383] FS-Cache: O-key=[10] '34323934393337343137'
[  +0.000263] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000342] FS-Cache: N-cookie d=00000000aed99636{9P.session} n=00000000df25c22a
[  +0.000335] FS-Cache: N-key=[10] '34323934393337343137'
[  +1.209820] FS-Cache: Duplicate cookie detected
[  +0.000453] FS-Cache: O-cookie c=00000008 [p=00000002 fl=222 nc=0 na=1]
[  +0.000913] FS-Cache: O-cookie d=00000000aed99636{9P.session} n=000000008d6cf06a
[  +0.001128] FS-Cache: O-key=[10] '34323934393337353338'
[  +0.000691] FS-Cache: N-cookie c=00000009 [p=00000002 fl=2 nc=0 na=1]
[  +0.000675] FS-Cache: N-cookie d=00000000aed99636{9P.session} n=0000000009cce3f2
[  +0.001305] FS-Cache: N-key=[10] '34323934393337353338'
[  +0.330334] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.014547] FS-Cache: Duplicate cookie detected
[  +0.000568] FS-Cache: O-cookie c=0000000e [p=00000002 fl=222 nc=0 na=1]
[  +0.000383] FS-Cache: O-cookie d=00000000aed99636{9P.session} n=00000000b1c69035
[  +0.000588] FS-Cache: O-key=[10] '34323934393337353733'
[  +0.000302] FS-Cache: N-cookie c=0000000f [p=00000002 fl=2 nc=0 na=1]
[  +0.000332] FS-Cache: N-cookie d=00000000aed99636{9P.session} n=00000000c54e02fb
[  +0.000388] FS-Cache: N-key=[10] '34323934393337353733'
[  +0.008689] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.189265] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000547] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000423] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000673] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002049] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000908] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000482] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001088] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +2.075355] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001396] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.000740] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.002347] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.006362] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001464] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.002209] WSL (4) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.000926] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.012987] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.112423] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.005613] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.006403] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.007378] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001183] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.003712] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.009275] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.007241] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.448600] netlink: 'init': attribute type 4 has an invalid length.


==> etcd [08503fcf66b0] <==
{"level":"info","ts":"2025-01-03T20:07:52.65035Z","caller":"traceutil/trace.go:171","msg":"trace[474945136] linearizableReadLoop","detail":"{readStateIndex:24; appliedIndex:16; }","duration":"124.906916ms","start":"2025-01-03T20:07:52.525431Z","end":"2025-01-03T20:07:52.650337Z","steps":["trace[474945136] 'read index received'  (duration: 80.021784ms)","trace[474945136] 'applied index is now lower than readState.Index'  (duration: 44.88423ms)"],"step_count":2}
{"level":"info","ts":"2025-01-03T20:07:52.650368Z","caller":"traceutil/trace.go:171","msg":"trace[193043451] transaction","detail":"{read_only:false; response_revision:19; number_of_response:1; }","duration":"154.850051ms","start":"2025-01-03T20:07:52.495498Z","end":"2025-01-03T20:07:52.650349Z","steps":["trace[193043451] 'process raft request'  (duration: 154.745283ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:52.650468Z","caller":"traceutil/trace.go:171","msg":"trace[894876494] transaction","detail":"{read_only:false; response_revision:20; number_of_response:1; }","duration":"144.962623ms","start":"2025-01-03T20:07:52.505494Z","end":"2025-01-03T20:07:52.650457Z","steps":["trace[894876494] 'process raft request'  (duration: 144.787168ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:52.650456Z","caller":"traceutil/trace.go:171","msg":"trace[462328921] transaction","detail":"{read_only:false; response_revision:16; number_of_response:1; }","duration":"156.885943ms","start":"2025-01-03T20:07:52.49342Z","end":"2025-01-03T20:07:52.650305Z","steps":["trace[462328921] 'process raft request'  (duration: 156.714816ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:52.650585Z","caller":"traceutil/trace.go:171","msg":"trace[1816194167] transaction","detail":"{read_only:false; response_revision:18; number_of_response:1; }","duration":"155.201529ms","start":"2025-01-03T20:07:52.495375Z","end":"2025-01-03T20:07:52.650577Z","steps":["trace[1816194167] 'process raft request'  (duration: 154.817196ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:52.650696Z","caller":"traceutil/trace.go:171","msg":"trace[541131172] transaction","detail":"{read_only:false; response_revision:15; number_of_response:1; }","duration":"157.997357ms","start":"2025-01-03T20:07:52.492687Z","end":"2025-01-03T20:07:52.650684Z","steps":["trace[541131172] 'process raft request'  (duration: 157.363931ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:52.650551Z","caller":"traceutil/trace.go:171","msg":"trace[260396578] transaction","detail":"{read_only:false; response_revision:17; number_of_response:1; }","duration":"156.320022ms","start":"2025-01-03T20:07:52.49422Z","end":"2025-01-03T20:07:52.65054Z","steps":["trace[260396578] 'process raft request'  (duration: 155.932376ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-03T20:07:52.650617Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"154.99235ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/default\" ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-01-03T20:07:52.650849Z","caller":"traceutil/trace.go:171","msg":"trace[1492411884] range","detail":"{range_begin:/registry/namespaces/default; range_end:; response_count:0; response_revision:20; }","duration":"155.266758ms","start":"2025-01-03T20:07:52.495568Z","end":"2025-01-03T20:07:52.650835Z","steps":["trace[1492411884] 'agreement among raft nodes before linearized reading'  (duration: 154.998309ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-03T20:07:52.652747Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.61337ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/kube-system\" ","response":"range_response_count:1 size:350"}
{"level":"info","ts":"2025-01-03T20:07:52.652843Z","caller":"traceutil/trace.go:171","msg":"trace[663805059] range","detail":"{range_begin:/registry/namespaces/kube-system; range_end:; response_count:1; response_revision:23; }","duration":"113.797765ms","start":"2025-01-03T20:07:52.53902Z","end":"2025-01-03T20:07:52.652817Z","steps":["trace[663805059] 'agreement among raft nodes before linearized reading'  (duration: 113.640364ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:52.653286Z","caller":"traceutil/trace.go:171","msg":"trace[469301684] transaction","detail":"{read_only:false; response_revision:21; number_of_response:1; }","duration":"123.108714ms","start":"2025-01-03T20:07:52.530159Z","end":"2025-01-03T20:07:52.653268Z","steps":["trace[469301684] 'process raft request'  (duration: 122.264093ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:52.653524Z","caller":"traceutil/trace.go:171","msg":"trace[976094892] transaction","detail":"{read_only:false; response_revision:22; number_of_response:1; }","duration":"123.008651ms","start":"2025-01-03T20:07:52.530503Z","end":"2025-01-03T20:07:52.653512Z","steps":["trace[976094892] 'process raft request'  (duration: 122.04523ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-03T20:07:52.780913Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.53465ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/kube-system/extension-apiserver-authentication\" ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-01-03T20:07:52.781021Z","caller":"traceutil/trace.go:171","msg":"trace[205485642] range","detail":"{range_begin:/registry/configmaps/kube-system/extension-apiserver-authentication; range_end:; response_count:0; response_revision:32; }","duration":"103.759962ms","start":"2025-01-03T20:07:52.677233Z","end":"2025-01-03T20:07:52.780993Z","steps":["trace[205485642] 'agreement among raft nodes before linearized reading'  (duration: 103.506096ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-03T20:07:52.781008Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.328003ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-01-03T20:07:52.781021Z","caller":"traceutil/trace.go:171","msg":"trace[1843309611] transaction","detail":"{read_only:false; response_revision:32; number_of_response:1; }","duration":"103.378014ms","start":"2025-01-03T20:07:52.677614Z","end":"2025-01-03T20:07:52.780992Z","steps":["trace[1843309611] 'process raft request'  (duration: 100.052066ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:52.781085Z","caller":"traceutil/trace.go:171","msg":"trace[219097074] range","detail":"{range_begin:/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii; range_end:; response_count:0; response_revision:32; }","duration":"101.449426ms","start":"2025-01-03T20:07:52.679614Z","end":"2025-01-03T20:07:52.781064Z","steps":["trace[219097074] 'agreement among raft nodes before linearized reading'  (duration: 101.315429ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:52.843765Z","caller":"traceutil/trace.go:171","msg":"trace[761296108] transaction","detail":"{read_only:false; response_revision:37; number_of_response:1; }","duration":"101.059333ms","start":"2025-01-03T20:07:52.74268Z","end":"2025-01-03T20:07:52.84374Z","steps":["trace[761296108] 'process raft request'  (duration: 101.01995ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:52.843838Z","caller":"traceutil/trace.go:171","msg":"trace[1061784237] transaction","detail":"{read_only:false; response_revision:33; number_of_response:1; }","duration":"102.421124ms","start":"2025-01-03T20:07:52.74139Z","end":"2025-01-03T20:07:52.843811Z","steps":["trace[1061784237] 'process raft request'  (duration: 102.06657ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:52.844063Z","caller":"traceutil/trace.go:171","msg":"trace[551806414] transaction","detail":"{read_only:false; response_revision:35; number_of_response:1; }","duration":"101.386565ms","start":"2025-01-03T20:07:52.742652Z","end":"2025-01-03T20:07:52.844039Z","steps":["trace[551806414] 'process raft request'  (duration: 100.990851ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:52.844162Z","caller":"traceutil/trace.go:171","msg":"trace[2052511739] transaction","detail":"{read_only:false; response_revision:34; number_of_response:1; }","duration":"101.504907ms","start":"2025-01-03T20:07:52.742646Z","end":"2025-01-03T20:07:52.844151Z","steps":["trace[2052511739] 'process raft request'  (duration: 100.966862ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:52.84406Z","caller":"traceutil/trace.go:171","msg":"trace[954723017] transaction","detail":"{read_only:false; response_revision:36; number_of_response:1; }","duration":"101.380065ms","start":"2025-01-03T20:07:52.742664Z","end":"2025-01-03T20:07:52.844044Z","steps":["trace[954723017] 'process raft request'  (duration: 101.004792ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:52.934902Z","caller":"traceutil/trace.go:171","msg":"trace[2077466071] transaction","detail":"{read_only:false; response_revision:38; number_of_response:1; }","duration":"149.333603ms","start":"2025-01-03T20:07:52.785538Z","end":"2025-01-03T20:07:52.934872Z","steps":["trace[2077466071] 'process raft request'  (duration: 117.717769ms)","trace[2077466071] 'compare'  (duration: 31.134221ms)"],"step_count":2}
{"level":"info","ts":"2025-01-03T20:07:52.93514Z","caller":"traceutil/trace.go:171","msg":"trace[2043326297] transaction","detail":"{read_only:false; response_revision:39; number_of_response:1; }","duration":"149.261867ms","start":"2025-01-03T20:07:52.78586Z","end":"2025-01-03T20:07:52.935122Z","steps":["trace[2043326297] 'process raft request'  (duration: 149.033048ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:53.407596Z","caller":"traceutil/trace.go:171","msg":"trace[2007030911] transaction","detail":"{read_only:false; response_revision:60; number_of_response:1; }","duration":"100.337678ms","start":"2025-01-03T20:07:53.307237Z","end":"2025-01-03T20:07:53.407575Z","steps":["trace[2007030911] 'process raft request'  (duration: 100.296254ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:53.407843Z","caller":"traceutil/trace.go:171","msg":"trace[2130132775] linearizableReadLoop","detail":"{readStateIndex:64; appliedIndex:63; }","duration":"104.577692ms","start":"2025-01-03T20:07:53.303166Z","end":"2025-01-03T20:07:53.407744Z","steps":["trace[2130132775] 'read index received'  (duration: 22.077372ms)","trace[2130132775] 'applied index is now lower than readState.Index'  (duration: 82.495022ms)"],"step_count":2}
{"level":"info","ts":"2025-01-03T20:07:53.40799Z","caller":"traceutil/trace.go:171","msg":"trace[1512009843] transaction","detail":"{read_only:false; response_revision:59; number_of_response:1; }","duration":"137.107407ms","start":"2025-01-03T20:07:53.270867Z","end":"2025-01-03T20:07:53.407975Z","steps":["trace[1512009843] 'process raft request'  (duration: 54.353287ms)","trace[1512009843] 'compare'  (duration: 82.176533ms)"],"step_count":2}
{"level":"warn","ts":"2025-01-03T20:07:53.408207Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.28177ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/edit\" ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-01-03T20:07:53.408331Z","caller":"traceutil/trace.go:171","msg":"trace[2074245693] range","detail":"{range_begin:/registry/clusterroles/edit; range_end:; response_count:0; response_revision:60; }","duration":"101.491975ms","start":"2025-01-03T20:07:53.306824Z","end":"2025-01-03T20:07:53.408316Z","steps":["trace[2074245693] 'agreement among raft nodes before linearized reading'  (duration: 101.218758ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-03T20:07:53.408261Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"137.412146ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/system-leader-election\" ","response":"range_response_count:1 size:1213"}
{"level":"info","ts":"2025-01-03T20:07:53.408419Z","caller":"traceutil/trace.go:171","msg":"trace[1553737813] range","detail":"{range_begin:/registry/flowschemas/system-leader-election; range_end:; response_count:1; response_revision:60; }","duration":"137.604534ms","start":"2025-01-03T20:07:53.270801Z","end":"2025-01-03T20:07:53.408405Z","steps":["trace[1553737813] 'agreement among raft nodes before linearized reading'  (duration: 137.303121ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:53.849684Z","caller":"traceutil/trace.go:171","msg":"trace[907071944] linearizableReadLoop","detail":"{readStateIndex:92; appliedIndex:91; }","duration":"142.850363ms","start":"2025-01-03T20:07:53.706806Z","end":"2025-01-03T20:07:53.849657Z","steps":["trace[907071944] 'read index received'  (duration: 80.278056ms)","trace[907071944] 'applied index is now lower than readState.Index'  (duration: 62.569633ms)"],"step_count":2}
{"level":"info","ts":"2025-01-03T20:07:53.849626Z","caller":"traceutil/trace.go:171","msg":"trace[1012182896] transaction","detail":"{read_only:false; response_revision:88; number_of_response:1; }","duration":"142.760142ms","start":"2025-01-03T20:07:53.706849Z","end":"2025-01-03T20:07:53.849609Z","steps":["trace[1012182896] 'process raft request'  (duration: 142.625427ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:53.849687Z","caller":"traceutil/trace.go:171","msg":"trace[122915264] transaction","detail":"{read_only:false; response_revision:87; number_of_response:1; }","duration":"170.605523ms","start":"2025-01-03T20:07:53.678981Z","end":"2025-01-03T20:07:53.849586Z","steps":["trace[122915264] 'process raft request'  (duration: 108.113067ms)","trace[122915264] 'compare'  (duration: 62.199093ms)"],"step_count":2}
{"level":"warn","ts":"2025-01-03T20:07:53.849884Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"143.003768ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/minikube.181747904ac82746\" ","response":"range_response_count:1 size:654"}
{"level":"info","ts":"2025-01-03T20:07:53.850092Z","caller":"traceutil/trace.go:171","msg":"trace[328563406] range","detail":"{range_begin:/registry/events/default/minikube.181747904ac82746; range_end:; response_count:1; response_revision:88; }","duration":"143.288032ms","start":"2025-01-03T20:07:53.70677Z","end":"2025-01-03T20:07:53.850058Z","steps":["trace[328563406] 'agreement among raft nodes before linearized reading'  (duration: 142.972584ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:07:56.258294Z","caller":"traceutil/trace.go:171","msg":"trace[1266187281] transaction","detail":"{read_only:false; response_revision:164; number_of_response:1; }","duration":"118.75567ms","start":"2025-01-03T20:07:56.139491Z","end":"2025-01-03T20:07:56.258246Z","steps":["trace[1266187281] 'process raft request'  (duration: 70.368611ms)","trace[1266187281] 'compare'  (duration: 48.183104ms)"],"step_count":2}
{"level":"warn","ts":"2025-01-03T20:08:22.994226Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"120.039917ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128034351846569875 > lease_revoke:<id:70cc942dc7fcb60d>","response":"size:29"}
{"level":"info","ts":"2025-01-03T20:08:56.349084Z","caller":"traceutil/trace.go:171","msg":"trace[1695096388] transaction","detail":"{read_only:false; response_revision:650; number_of_response:1; }","duration":"108.799147ms","start":"2025-01-03T20:08:56.240247Z","end":"2025-01-03T20:08:56.349046Z","steps":["trace[1695096388] 'process raft request'  (duration: 36.43074ms)","trace[1695096388] 'compare'  (duration: 72.180398ms)"],"step_count":2}
{"level":"warn","ts":"2025-01-03T20:08:56.584835Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"132.408567ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/default/storage-prometheus-alertmanager-0\" ","response":"range_response_count:1 size:1378"}
{"level":"info","ts":"2025-01-03T20:08:56.58497Z","caller":"traceutil/trace.go:171","msg":"trace[907097786] range","detail":"{range_begin:/registry/persistentvolumeclaims/default/storage-prometheus-alertmanager-0; range_end:; response_count:1; response_revision:650; }","duration":"132.585434ms","start":"2025-01-03T20:08:56.452341Z","end":"2025-01-03T20:08:56.584927Z","steps":["trace[907097786] 'range keys from in-memory index tree'  (duration: 131.913301ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-03T20:08:59.619592Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"120.652871ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/certificatesigningrequests/\" range_end:\"/registry/certificatesigningrequests0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-01-03T20:08:59.619765Z","caller":"traceutil/trace.go:171","msg":"trace[737045797] range","detail":"{range_begin:/registry/certificatesigningrequests/; range_end:/registry/certificatesigningrequests0; response_count:0; response_revision:653; }","duration":"120.89518ms","start":"2025-01-03T20:08:59.49884Z","end":"2025-01-03T20:08:59.619735Z","steps":["trace[737045797] 'count revisions from in-memory index tree'  (duration: 120.556733ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:08:59.741149Z","caller":"traceutil/trace.go:171","msg":"trace[2081824629] linearizableReadLoop","detail":"{readStateIndex:680; appliedIndex:679; }","duration":"136.43519ms","start":"2025-01-03T20:08:59.604692Z","end":"2025-01-03T20:08:59.741127Z","steps":["trace[2081824629] 'read index received'  (duration: 136.171417ms)","trace[2081824629] 'applied index is now lower than readState.Index'  (duration: 262.871µs)"],"step_count":2}
{"level":"info","ts":"2025-01-03T20:08:59.741338Z","caller":"traceutil/trace.go:171","msg":"trace[662756428] transaction","detail":"{read_only:false; response_revision:654; number_of_response:1; }","duration":"160.144465ms","start":"2025-01-03T20:08:59.581171Z","end":"2025-01-03T20:08:59.741316Z","steps":["trace[662756428] 'process raft request'  (duration: 159.658951ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-03T20:08:59.741417Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"136.698298ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:600"}
{"level":"info","ts":"2025-01-03T20:08:59.741558Z","caller":"traceutil/trace.go:171","msg":"trace[142743873] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:654; }","duration":"136.890979ms","start":"2025-01-03T20:08:59.604646Z","end":"2025-01-03T20:08:59.741537Z","steps":["trace[142743873] 'agreement among raft nodes before linearized reading'  (duration: 136.594213ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:08:59.962974Z","caller":"traceutil/trace.go:171","msg":"trace[1412868931] transaction","detail":"{read_only:false; response_revision:655; number_of_response:1; }","duration":"214.937464ms","start":"2025-01-03T20:08:59.747998Z","end":"2025-01-03T20:08:59.962935Z","steps":["trace[1412868931] 'process raft request'  (duration: 133.326283ms)","trace[1412868931] 'compare'  (duration: 81.44845ms)"],"step_count":2}
{"level":"info","ts":"2025-01-03T20:09:16.687372Z","caller":"traceutil/trace.go:171","msg":"trace[1311118330] transaction","detail":"{read_only:false; response_revision:673; number_of_response:1; }","duration":"230.605499ms","start":"2025-01-03T20:09:16.456738Z","end":"2025-01-03T20:09:16.687343Z","steps":["trace[1311118330] 'process raft request'  (duration: 230.293458ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-03T20:09:16.971254Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"165.269477ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128034351846570206 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/specs/default/static-web-app-service\" mod_revision:0 > success:<request_put:<key:\"/registry/services/specs/default/static-web-app-service\" value_size:1010 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2025-01-03T20:09:16.971409Z","caller":"traceutil/trace.go:171","msg":"trace[789093304] linearizableReadLoop","detail":"{readStateIndex:704; appliedIndex:703; }","duration":"139.709595ms","start":"2025-01-03T20:09:16.831681Z","end":"2025-01-03T20:09:16.97139Z","steps":["trace[789093304] 'read index received'  (duration: 82.864µs)","trace[789093304] 'applied index is now lower than readState.Index'  (duration: 139.625215ms)"],"step_count":2}
{"level":"info","ts":"2025-01-03T20:09:16.971421Z","caller":"traceutil/trace.go:171","msg":"trace[742867381] transaction","detail":"{read_only:false; response_revision:675; number_of_response:1; }","duration":"260.820874ms","start":"2025-01-03T20:09:16.712529Z","end":"2025-01-03T20:09:16.971392Z","steps":["trace[742867381] 'process raft request'  (duration: 94.86388ms)","trace[742867381] 'compare'  (duration: 165.097167ms)"],"step_count":2}
{"level":"warn","ts":"2025-01-03T20:09:16.971526Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"139.831996ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/\" range_end:\"/registry/services/endpoints0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-01-03T20:09:16.971599Z","caller":"traceutil/trace.go:171","msg":"trace[187328852] range","detail":"{range_begin:/registry/services/endpoints/; range_end:/registry/services/endpoints0; response_count:0; response_revision:675; }","duration":"139.970604ms","start":"2025-01-03T20:09:16.831616Z","end":"2025-01-03T20:09:16.971587Z","steps":["trace[187328852] 'agreement among raft nodes before linearized reading'  (duration: 139.828688ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:09:17.134394Z","caller":"traceutil/trace.go:171","msg":"trace[1143521188] transaction","detail":"{read_only:false; response_revision:677; number_of_response:1; }","duration":"157.418408ms","start":"2025-01-03T20:09:16.976952Z","end":"2025-01-03T20:09:17.13437Z","steps":["trace[1143521188] 'process raft request'  (duration: 108.466793ms)","trace[1143521188] 'compare'  (duration: 48.744451ms)"],"step_count":2}
{"level":"info","ts":"2025-01-03T20:16:19.484143Z","caller":"traceutil/trace.go:171","msg":"trace[1790918759] transaction","detail":"{read_only:false; response_revision:1027; number_of_response:1; }","duration":"106.735209ms","start":"2025-01-03T20:16:19.377385Z","end":"2025-01-03T20:16:19.48412Z","steps":["trace[1790918759] 'process raft request'  (duration: 106.543168ms)"],"step_count":1}
{"level":"info","ts":"2025-01-03T20:17:49.944993Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":860}
{"level":"info","ts":"2025-01-03T20:17:49.961614Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":860,"took":"16.251112ms","hash":3343854493,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":3170304,"current-db-size-in-use":"3.2 MB"}
{"level":"info","ts":"2025-01-03T20:17:49.961693Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3343854493,"revision":860,"compact-revision":-1}


==> kernel <==
 20:19:10 up  1:23,  0 users,  load average: 0.36, 0.73, 0.58
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [a466d7953f8c] <==
I0103 20:07:52.100400       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0103 20:07:52.100414       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0103 20:07:52.100417       1 controller.go:78] Starting OpenAPI AggregationController
I0103 20:07:52.100760       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0103 20:07:52.100811       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0103 20:07:52.100876       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0103 20:07:52.100969       1 controller.go:139] Starting OpenAPI controller
I0103 20:07:52.101025       1 controller.go:87] Starting OpenAPI V3 controller
I0103 20:07:52.101045       1 naming_controller.go:291] Starting NamingConditionController
I0103 20:07:52.101065       1 establishing_controller.go:76] Starting EstablishingController
I0103 20:07:52.101098       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0103 20:07:52.101123       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0103 20:07:52.101145       1 crd_finalizer.go:266] Starting CRDFinalizer
I0103 20:07:52.101211       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0103 20:07:52.101229       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0103 20:07:52.101369       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0103 20:07:52.101487       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0103 20:07:52.256624       1 shared_informer.go:320] Caches are synced for configmaps
I0103 20:07:52.256927       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0103 20:07:52.256936       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0103 20:07:52.328559       1 shared_informer.go:320] Caches are synced for node_authorizer
I0103 20:07:52.328573       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0103 20:07:52.328603       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0103 20:07:52.329851       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0103 20:07:52.329893       1 aggregator.go:165] initial CRD sync complete...
I0103 20:07:52.329907       1 autoregister_controller.go:141] Starting autoregister controller
I0103 20:07:52.329917       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0103 20:07:52.329927       1 cache.go:39] Caches are synced for autoregister controller
I0103 20:07:52.330532       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0103 20:07:52.330883       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0103 20:07:52.333534       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0103 20:07:52.333558       1 policy_source.go:224] refreshing policies
I0103 20:07:52.433333       1 controller.go:615] quota admission added evaluator for: namespaces
E0103 20:07:52.477797       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
E0103 20:07:52.478506       1 controller.go:145] while syncing ConfigMap "kube-system/kube-apiserver-legacy-service-account-token-tracking", err: namespaces "kube-system" not found
I0103 20:07:52.784879       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0103 20:07:53.211686       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0103 20:07:53.232763       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0103 20:07:53.232872       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0103 20:07:57.466506       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0103 20:07:57.643672       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0103 20:07:57.799339       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0103 20:07:57.818185       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0103 20:07:57.821884       1 controller.go:615] quota admission added evaluator for: endpoints
I0103 20:07:57.832929       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0103 20:07:58.155710       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0103 20:07:58.765413       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0103 20:07:58.852015       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0103 20:07:58.882226       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0103 20:08:12.251142       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0103 20:08:12.319537       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0103 20:08:15.598098       1 alloc.go:330] "allocated clusterIPs" service="default/prometheus-prometheus-node-exporter" clusterIPs={"IPv4":"10.103.37.226"}
I0103 20:08:15.636877       1 alloc.go:330] "allocated clusterIPs" service="default/prometheus-kube-state-metrics" clusterIPs={"IPv4":"10.106.19.177"}
I0103 20:08:15.652735       1 alloc.go:330] "allocated clusterIPs" service="default/prometheus-alertmanager" clusterIPs={"IPv4":"10.104.104.3"}
I0103 20:08:15.672180       1 alloc.go:330] "allocated clusterIPs" service="default/prometheus-server" clusterIPs={"IPv4":"10.111.241.174"}
I0103 20:08:15.688411       1 alloc.go:330] "allocated clusterIPs" service="default/prometheus-prometheus-pushgateway" clusterIPs={"IPv4":"10.96.154.67"}
I0103 20:08:15.732562       1 controller.go:615] quota admission added evaluator for: statefulsets.apps
I0103 20:09:16.972105       1 alloc.go:330] "allocated clusterIPs" service="default/static-web-app-service" clusterIPs={"IPv4":"10.110.68.154"}
I0103 20:09:16.972517       1 trace.go:236] Trace[1576895750]: "Create" accept:application/json,audit-id:52df9edf-95f9-4926-b570-845bf12ee807,client:192.168.49.1,api-group:,api-version:v1,name:,subresource:,namespace:default,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/default/services,user-agent:kubectl.exe/v1.29.2 (windows/amd64) kubernetes/4b8e819,verb:POST (03-Jan-2025 20:09:16.449) (total time: 524ms):
Trace[1576895750]: [524.514901ms] [524.514901ms] END


==> kube-controller-manager [68f35cc3b843] <==
I0103 20:08:11.533523       1 shared_informer.go:320] Caches are synced for attach detach
I0103 20:08:11.537840       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0103 20:08:11.544032       1 shared_informer.go:320] Caches are synced for PVC protection
I0103 20:08:11.549592       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0103 20:08:11.549727       1 shared_informer.go:320] Caches are synced for HPA
I0103 20:08:11.550968       1 shared_informer.go:320] Caches are synced for daemon sets
I0103 20:08:11.551103       1 shared_informer.go:320] Caches are synced for deployment
I0103 20:08:11.551298       1 shared_informer.go:320] Caches are synced for GC
I0103 20:08:11.552495       1 shared_informer.go:320] Caches are synced for ephemeral
I0103 20:08:11.554103       1 shared_informer.go:320] Caches are synced for stateful set
I0103 20:08:11.555835       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0103 20:08:11.562430       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0103 20:08:11.564814       1 shared_informer.go:320] Caches are synced for endpoint
I0103 20:08:11.566247       1 shared_informer.go:320] Caches are synced for disruption
I0103 20:08:11.574401       1 shared_informer.go:320] Caches are synced for resource quota
I0103 20:08:11.579351       1 shared_informer.go:320] Caches are synced for persistent volume
I0103 20:08:11.599055       1 shared_informer.go:320] Caches are synced for job
I0103 20:08:11.606190       1 shared_informer.go:320] Caches are synced for resource quota
I0103 20:08:11.608519       1 shared_informer.go:320] Caches are synced for ReplicationController
I0103 20:08:12.016261       1 shared_informer.go:320] Caches are synced for garbage collector
I0103 20:08:12.044810       1 shared_informer.go:320] Caches are synced for garbage collector
I0103 20:08:12.044876       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0103 20:08:12.615585       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="331.917509ms"
I0103 20:08:12.689263       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-app-deployment-78c4d59b57" duration="413.414376ms"
I0103 20:08:12.728160       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="112.511015ms"
I0103 20:08:12.728233       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-app-deployment-78c4d59b57" duration="38.927155ms"
I0103 20:08:12.728292       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-app-deployment-78c4d59b57" duration="37.287µs"
I0103 20:08:12.728320       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-app-deployment-78c4d59b57" duration="15.43µs"
I0103 20:08:12.796286       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-app-deployment-78c4d59b57" duration="159.183µs"
I0103 20:08:12.802803       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="74.580132ms"
I0103 20:08:12.802956       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="72.538µs"
I0103 20:08:15.380702       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="29.143528ms"
I0103 20:08:15.380948       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="61.092µs"
I0103 20:08:15.793069       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-kube-state-metrics-8ff4967cb" duration="38.283085ms"
I0103 20:08:15.795057       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-7d7b6497cd" duration="40.394903ms"
I0103 20:08:15.834097       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-server-9976468d9" duration="79.393594ms"
I0103 20:08:15.873379       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-kube-state-metrics-8ff4967cb" duration="44.147864ms"
I0103 20:08:15.873887       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-kube-state-metrics-8ff4967cb" duration="125.908µs"
I0103 20:08:15.928422       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-7d7b6497cd" duration="98.641782ms"
I0103 20:08:15.928507       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-server-9976468d9" duration="94.314699ms"
I0103 20:08:15.928926       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-7d7b6497cd" duration="91.001µs"
I0103 20:08:15.929288       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-server-9976468d9" duration="82.323µs"
I0103 20:08:15.953691       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-7d7b6497cd" duration="259.846µs"
I0103 20:08:15.987684       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-kube-state-metrics-8ff4967cb" duration="71.386µs"
I0103 20:08:23.758018       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-app-deployment-78c4d59b57" duration="6.219123ms"
I0103 20:08:23.758116       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-app-deployment-78c4d59b57" duration="43.809µs"
I0103 20:08:25.833909       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-app-deployment-78c4d59b57" duration="29.080425ms"
I0103 20:08:25.833997       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/web-app-deployment-78c4d59b57" duration="33.015µs"
I0103 20:08:36.359727       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-server-9976468d9" duration="68.168µs"
I0103 20:08:36.392638       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-server-9976468d9" duration="164.487µs"
I0103 20:08:40.121426       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-7d7b6497cd" duration="66.19µs"
I0103 20:08:46.243800       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-kube-state-metrics-8ff4967cb" duration="59.719µs"
I0103 20:08:56.184622       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-7d7b6497cd" duration="15.558962ms"
I0103 20:08:56.184735       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-7d7b6497cd" duration="59.525µs"
I0103 20:08:56.231125       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-kube-state-metrics-8ff4967cb" duration="46.257481ms"
I0103 20:08:56.231346       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-kube-state-metrics-8ff4967cb" duration="122.593µs"
I0103 20:09:19.765238       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-server-9976468d9" duration="74.647µs"
I0103 20:09:19.802116       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-server-9976468d9" duration="95.423µs"
I0103 20:09:51.696204       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-server-9976468d9" duration="15.343601ms"
I0103 20:09:51.696331       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/prometheus-server-9976468d9" duration="73.593µs"


==> kube-proxy [df5419833415] <==
I0103 20:08:14.637250       1 server_linux.go:69] "Using iptables proxy"
I0103 20:08:14.648560       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0103 20:08:14.678659       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0103 20:08:14.678779       1 server_linux.go:165] "Using iptables Proxier"
I0103 20:08:14.681380       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0103 20:08:14.681425       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0103 20:08:14.681458       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0103 20:08:14.681860       1 server.go:872] "Version info" version="v1.30.0"
I0103 20:08:14.681904       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0103 20:08:14.684763       1 config.go:192] "Starting service config controller"
I0103 20:08:14.684812       1 shared_informer.go:313] Waiting for caches to sync for service config
I0103 20:08:14.684838       1 config.go:101] "Starting endpoint slice config controller"
I0103 20:08:14.684843       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0103 20:08:14.684850       1 config.go:319] "Starting node config controller"
I0103 20:08:14.684876       1 shared_informer.go:313] Waiting for caches to sync for node config
I0103 20:08:14.784972       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0103 20:08:14.785014       1 shared_informer.go:320] Caches are synced for node config
I0103 20:08:14.784981       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [ccd3894d93a8] <==
W0103 20:07:52.369185       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0103 20:07:52.369211       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0103 20:07:52.369213       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0103 20:07:52.369263       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0103 20:07:52.369294       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0103 20:07:53.190907       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0103 20:07:53.190954       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0103 20:07:53.272839       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0103 20:07:53.272895       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0103 20:07:53.505135       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0103 20:07:53.505230       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0103 20:07:53.510039       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0103 20:07:53.510130       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0103 20:07:53.511343       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0103 20:07:53.511466       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0103 20:07:53.511356       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0103 20:07:53.511521       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0103 20:07:53.549177       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0103 20:07:53.549283       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0103 20:07:53.681906       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0103 20:07:53.681950       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0103 20:07:53.723865       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0103 20:07:53.723938       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0103 20:07:53.741428       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0103 20:07:53.741503       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0103 20:07:53.745047       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0103 20:07:53.745128       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0103 20:07:53.841036       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0103 20:07:53.841197       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0103 20:07:53.848210       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0103 20:07:53.848279       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0103 20:07:53.911126       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0103 20:07:53.911212       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0103 20:07:54.024863       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0103 20:07:54.024932       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0103 20:07:54.932233       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0103 20:07:54.932278       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0103 20:07:55.368856       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0103 20:07:55.368902       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0103 20:07:55.614388       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0103 20:07:55.614437       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0103 20:07:55.700161       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0103 20:07:55.700266       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0103 20:07:55.826212       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0103 20:07:55.826324       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0103 20:07:55.917502       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0103 20:07:55.917682       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0103 20:07:56.042835       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0103 20:07:56.042935       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0103 20:07:56.056338       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0103 20:07:56.056437       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0103 20:07:56.070328       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0103 20:07:56.070430       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0103 20:07:56.079798       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0103 20:07:56.079903       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0103 20:07:56.252329       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0103 20:07:56.252412       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0103 20:07:56.493778       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0103 20:07:56.493839       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
I0103 20:08:00.157320       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Jan 03 20:08:11 minikube kubelet[2427]: I0103 20:08:11.619711    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tmd9f\" (UniqueName: \"kubernetes.io/projected/ca4728d1-c507-4f15-b5cb-12d5985b31bb-kube-api-access-tmd9f\") pod \"storage-provisioner\" (UID: \"ca4728d1-c507-4f15-b5cb-12d5985b31bb\") " pod="kube-system/storage-provisioner"
Jan 03 20:08:11 minikube kubelet[2427]: I0103 20:08:11.619787    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/ca4728d1-c507-4f15-b5cb-12d5985b31bb-tmp\") pod \"storage-provisioner\" (UID: \"ca4728d1-c507-4f15-b5cb-12d5985b31bb\") " pod="kube-system/storage-provisioner"
Jan 03 20:08:11 minikube kubelet[2427]: E0103 20:08:11.725958    2427 projected.go:294] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Jan 03 20:08:11 minikube kubelet[2427]: E0103 20:08:11.726009    2427 projected.go:200] Error preparing data for projected volume kube-api-access-tmd9f for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Jan 03 20:08:11 minikube kubelet[2427]: E0103 20:08:11.726114    2427 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/ca4728d1-c507-4f15-b5cb-12d5985b31bb-kube-api-access-tmd9f podName:ca4728d1-c507-4f15-b5cb-12d5985b31bb nodeName:}" failed. No retries permitted until 2025-01-03 20:08:12.22607759 +0000 UTC m=+13.649961420 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-tmd9f" (UniqueName: "kubernetes.io/projected/ca4728d1-c507-4f15-b5cb-12d5985b31bb-kube-api-access-tmd9f") pod "storage-provisioner" (UID: "ca4728d1-c507-4f15-b5cb-12d5985b31bb") : configmap "kube-root-ca.crt" not found
Jan 03 20:08:12 minikube kubelet[2427]: I0103 20:08:12.364576    2427 topology_manager.go:215] "Topology Admit Handler" podUID="ba283a7e-1231-40f3-8adb-ef4b02cdf147" podNamespace="kube-system" podName="kube-proxy-726vc"
Jan 03 20:08:12 minikube kubelet[2427]: I0103 20:08:12.526032    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/ba283a7e-1231-40f3-8adb-ef4b02cdf147-kube-proxy\") pod \"kube-proxy-726vc\" (UID: \"ba283a7e-1231-40f3-8adb-ef4b02cdf147\") " pod="kube-system/kube-proxy-726vc"
Jan 03 20:08:12 minikube kubelet[2427]: I0103 20:08:12.526122    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/ba283a7e-1231-40f3-8adb-ef4b02cdf147-xtables-lock\") pod \"kube-proxy-726vc\" (UID: \"ba283a7e-1231-40f3-8adb-ef4b02cdf147\") " pod="kube-system/kube-proxy-726vc"
Jan 03 20:08:12 minikube kubelet[2427]: I0103 20:08:12.526149    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/ba283a7e-1231-40f3-8adb-ef4b02cdf147-lib-modules\") pod \"kube-proxy-726vc\" (UID: \"ba283a7e-1231-40f3-8adb-ef4b02cdf147\") " pod="kube-system/kube-proxy-726vc"
Jan 03 20:08:12 minikube kubelet[2427]: I0103 20:08:12.526193    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-h2xmc\" (UniqueName: \"kubernetes.io/projected/ba283a7e-1231-40f3-8adb-ef4b02cdf147-kube-api-access-h2xmc\") pod \"kube-proxy-726vc\" (UID: \"ba283a7e-1231-40f3-8adb-ef4b02cdf147\") " pod="kube-system/kube-proxy-726vc"
Jan 03 20:08:12 minikube kubelet[2427]: I0103 20:08:12.615750    2427 topology_manager.go:215] "Topology Admit Handler" podUID="16f14e30-eba9-42e3-a072-e4401f96f509" podNamespace="kube-system" podName="coredns-7db6d8ff4d-lxkcs"
Jan 03 20:08:12 minikube kubelet[2427]: I0103 20:08:12.655138    2427 topology_manager.go:215] "Topology Admit Handler" podUID="d5e06bb7-ab57-40c4-b0dc-9b63ff987731" podNamespace="default" podName="web-app-deployment-78c4d59b57-cq6wq"
Jan 03 20:08:12 minikube kubelet[2427]: I0103 20:08:12.662906    2427 topology_manager.go:215] "Topology Admit Handler" podUID="ba183a2f-5551-42a2-86df-7634e0b5f711" podNamespace="default" podName="web-app-deployment-78c4d59b57-hbmmv"
Jan 03 20:08:12 minikube kubelet[2427]: I0103 20:08:12.727086    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-7mt8f\" (UniqueName: \"kubernetes.io/projected/16f14e30-eba9-42e3-a072-e4401f96f509-kube-api-access-7mt8f\") pod \"coredns-7db6d8ff4d-lxkcs\" (UID: \"16f14e30-eba9-42e3-a072-e4401f96f509\") " pod="kube-system/coredns-7db6d8ff4d-lxkcs"
Jan 03 20:08:12 minikube kubelet[2427]: I0103 20:08:12.727161    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/16f14e30-eba9-42e3-a072-e4401f96f509-config-volume\") pod \"coredns-7db6d8ff4d-lxkcs\" (UID: \"16f14e30-eba9-42e3-a072-e4401f96f509\") " pod="kube-system/coredns-7db6d8ff4d-lxkcs"
Jan 03 20:08:12 minikube kubelet[2427]: I0103 20:08:12.828717    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dcrbh\" (UniqueName: \"kubernetes.io/projected/d5e06bb7-ab57-40c4-b0dc-9b63ff987731-kube-api-access-dcrbh\") pod \"web-app-deployment-78c4d59b57-cq6wq\" (UID: \"d5e06bb7-ab57-40c4-b0dc-9b63ff987731\") " pod="default/web-app-deployment-78c4d59b57-cq6wq"
Jan 03 20:08:12 minikube kubelet[2427]: I0103 20:08:12.828857    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rwlj6\" (UniqueName: \"kubernetes.io/projected/ba183a2f-5551-42a2-86df-7634e0b5f711-kube-api-access-rwlj6\") pod \"web-app-deployment-78c4d59b57-hbmmv\" (UID: \"ba183a2f-5551-42a2-86df-7634e0b5f711\") " pod="default/web-app-deployment-78c4d59b57-hbmmv"
Jan 03 20:08:13 minikube kubelet[2427]: I0103 20:08:13.038057    2427 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="9437e63f4aa8905c0d81937d9310d8725ec139cd9a847419c8ee3e57eb20c3ac"
Jan 03 20:08:14 minikube kubelet[2427]: I0103 20:08:14.132175    2427 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="15f23bcf77ce768c432e6aa7d5841eeb70d9cc0b6c5d18cb998dc40c45338e62"
Jan 03 20:08:14 minikube kubelet[2427]: I0103 20:08:14.215531    2427 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a749f8f8fae09adf811839e33e4967319428d1e63aa54b50980082b4fe90e513"
Jan 03 20:08:14 minikube kubelet[2427]: I0103 20:08:14.231740    2427 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5823f69f5b36d6a649ea0266243c85b692875f0cd4d332e12545d27f070e3304"
Jan 03 20:08:14 minikube kubelet[2427]: I0103 20:08:14.245341    2427 scope.go:117] "RemoveContainer" containerID="25d6d1f8362de2ecddd8ca93546bd9d70f5741d6d7f06c39830b9b4579c9bd6c"
Jan 03 20:08:15 minikube kubelet[2427]: I0103 20:08:15.283979    2427 scope.go:117] "RemoveContainer" containerID="25d6d1f8362de2ecddd8ca93546bd9d70f5741d6d7f06c39830b9b4579c9bd6c"
Jan 03 20:08:15 minikube kubelet[2427]: I0103 20:08:15.284182    2427 scope.go:117] "RemoveContainer" containerID="05662dae44646ec9dad7ae299e3658feb0704f7a6df10284f51c18043f68761c"
Jan 03 20:08:15 minikube kubelet[2427]: E0103 20:08:15.284425    2427 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(ca4728d1-c507-4f15-b5cb-12d5985b31bb)\"" pod="kube-system/storage-provisioner" podUID="ca4728d1-c507-4f15-b5cb-12d5985b31bb"
Jan 03 20:08:15 minikube kubelet[2427]: I0103 20:08:15.333370    2427 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-726vc" podStartSLOduration=3.3333168779999998 podStartE2EDuration="3.333316878s" podCreationTimestamp="2025-01-03 20:08:12 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-01-03 20:08:15.288438733 +0000 UTC m=+16.712322569" watchObservedRunningTime="2025-01-03 20:08:15.333316878 +0000 UTC m=+16.757200719"
Jan 03 20:08:15 minikube kubelet[2427]: I0103 20:08:15.351302    2427 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-7db6d8ff4d-lxkcs" podStartSLOduration=3.3512691869999998 podStartE2EDuration="3.351269187s" podCreationTimestamp="2025-01-03 20:08:12 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-01-03 20:08:15.351190208 +0000 UTC m=+16.775074060" watchObservedRunningTime="2025-01-03 20:08:15.351269187 +0000 UTC m=+16.775153016"
Jan 03 20:08:15 minikube kubelet[2427]: I0103 20:08:15.758331    2427 topology_manager.go:215] "Topology Admit Handler" podUID="f10592c6-f247-4a8f-b175-59d20febce14" podNamespace="default" podName="prometheus-prometheus-node-exporter-lnj8b"
Jan 03 20:08:15 minikube kubelet[2427]: I0103 20:08:15.830523    2427 topology_manager.go:215] "Topology Admit Handler" podUID="ce40ab69-083d-435d-8b4f-f52b41be03ca" podNamespace="default" podName="prometheus-kube-state-metrics-8ff4967cb-stqs9"
Jan 03 20:08:15 minikube kubelet[2427]: I0103 20:08:15.831279    2427 topology_manager.go:215] "Topology Admit Handler" podUID="0987af24-a581-46a2-b26a-2ee52f8f2e57" podNamespace="default" podName="prometheus-prometheus-pushgateway-7d7b6497cd-mnl6z"
Jan 03 20:08:15 minikube kubelet[2427]: I0103 20:08:15.860968    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"root\" (UniqueName: \"kubernetes.io/host-path/f10592c6-f247-4a8f-b175-59d20febce14-root\") pod \"prometheus-prometheus-node-exporter-lnj8b\" (UID: \"f10592c6-f247-4a8f-b175-59d20febce14\") " pod="default/prometheus-prometheus-node-exporter-lnj8b"
Jan 03 20:08:15 minikube kubelet[2427]: I0103 20:08:15.861024    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"storage-volume\" (UniqueName: \"kubernetes.io/empty-dir/0987af24-a581-46a2-b26a-2ee52f8f2e57-storage-volume\") pod \"prometheus-prometheus-pushgateway-7d7b6497cd-mnl6z\" (UID: \"0987af24-a581-46a2-b26a-2ee52f8f2e57\") " pod="default/prometheus-prometheus-pushgateway-7d7b6497cd-mnl6z"
Jan 03 20:08:15 minikube kubelet[2427]: I0103 20:08:15.861043    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"proc\" (UniqueName: \"kubernetes.io/host-path/f10592c6-f247-4a8f-b175-59d20febce14-proc\") pod \"prometheus-prometheus-node-exporter-lnj8b\" (UID: \"f10592c6-f247-4a8f-b175-59d20febce14\") " pod="default/prometheus-prometheus-node-exporter-lnj8b"
Jan 03 20:08:15 minikube kubelet[2427]: I0103 20:08:15.861061    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"sys\" (UniqueName: \"kubernetes.io/host-path/f10592c6-f247-4a8f-b175-59d20febce14-sys\") pod \"prometheus-prometheus-node-exporter-lnj8b\" (UID: \"f10592c6-f247-4a8f-b175-59d20febce14\") " pod="default/prometheus-prometheus-node-exporter-lnj8b"
Jan 03 20:08:15 minikube kubelet[2427]: I0103 20:08:15.861082    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fdcvl\" (UniqueName: \"kubernetes.io/projected/ce40ab69-083d-435d-8b4f-f52b41be03ca-kube-api-access-fdcvl\") pod \"prometheus-kube-state-metrics-8ff4967cb-stqs9\" (UID: \"ce40ab69-083d-435d-8b4f-f52b41be03ca\") " pod="default/prometheus-kube-state-metrics-8ff4967cb-stqs9"
Jan 03 20:08:15 minikube kubelet[2427]: I0103 20:08:15.861130    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6xlx2\" (UniqueName: \"kubernetes.io/projected/0987af24-a581-46a2-b26a-2ee52f8f2e57-kube-api-access-6xlx2\") pod \"prometheus-prometheus-pushgateway-7d7b6497cd-mnl6z\" (UID: \"0987af24-a581-46a2-b26a-2ee52f8f2e57\") " pod="default/prometheus-prometheus-pushgateway-7d7b6497cd-mnl6z"
Jan 03 20:08:16 minikube kubelet[2427]: I0103 20:08:16.625346    2427 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="45fcf4e272695c05cd84b039772991d1a7b56948c5d3badb491a0aaa9bfa1c1b"
Jan 03 20:08:16 minikube kubelet[2427]: I0103 20:08:16.638947    2427 scope.go:117] "RemoveContainer" containerID="05662dae44646ec9dad7ae299e3658feb0704f7a6df10284f51c18043f68761c"
Jan 03 20:08:16 minikube kubelet[2427]: E0103 20:08:16.639391    2427 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(ca4728d1-c507-4f15-b5cb-12d5985b31bb)\"" pod="kube-system/storage-provisioner" podUID="ca4728d1-c507-4f15-b5cb-12d5985b31bb"
Jan 03 20:08:19 minikube kubelet[2427]: I0103 20:08:19.327847    2427 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jan 03 20:08:19 minikube kubelet[2427]: I0103 20:08:19.328877    2427 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jan 03 20:08:25 minikube kubelet[2427]: I0103 20:08:25.804871    2427 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/web-app-deployment-78c4d59b57-hbmmv" podStartSLOduration=6.129073866 podStartE2EDuration="13.804845562s" podCreationTimestamp="2025-01-03 20:08:12 +0000 UTC" firstStartedPulling="2025-01-03 20:08:14.541199947 +0000 UTC m=+15.965083773" lastFinishedPulling="2025-01-03 20:08:22.215307603 +0000 UTC m=+23.640855469" observedRunningTime="2025-01-03 20:08:23.751884985 +0000 UTC m=+25.177432857" watchObservedRunningTime="2025-01-03 20:08:25.804845562 +0000 UTC m=+27.230393438"
Jan 03 20:08:30 minikube kubelet[2427]: I0103 20:08:30.669427    2427 scope.go:117] "RemoveContainer" containerID="05662dae44646ec9dad7ae299e3658feb0704f7a6df10284f51c18043f68761c"
Jan 03 20:08:31 minikube kubelet[2427]: I0103 20:08:31.874607    2427 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/web-app-deployment-78c4d59b57-cq6wq" podStartSLOduration=9.694005767 podStartE2EDuration="19.874581026s" podCreationTimestamp="2025-01-03 20:08:12 +0000 UTC" firstStartedPulling="2025-01-03 20:08:14.541384669 +0000 UTC m=+15.965268495" lastFinishedPulling="2025-01-03 20:08:24.720295885 +0000 UTC m=+26.145843754" observedRunningTime="2025-01-03 20:08:25.805390927 +0000 UTC m=+27.230938800" watchObservedRunningTime="2025-01-03 20:08:31.874581026 +0000 UTC m=+33.300128895"
Jan 03 20:08:31 minikube kubelet[2427]: I0103 20:08:31.874908    2427 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=31.874900159 podStartE2EDuration="31.874900159s" podCreationTimestamp="2025-01-03 20:08:00 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-01-03 20:08:31.874515416 +0000 UTC m=+33.300063288" watchObservedRunningTime="2025-01-03 20:08:31.874900159 +0000 UTC m=+33.300448080"
Jan 03 20:08:32 minikube kubelet[2427]: I0103 20:08:32.915691    2427 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/prometheus-prometheus-node-exporter-lnj8b" podStartSLOduration=2.371811842 podStartE2EDuration="17.915665492s" podCreationTimestamp="2025-01-03 20:08:15 +0000 UTC" firstStartedPulling="2025-01-03 20:08:16.628757833 +0000 UTC m=+18.052641682" lastFinishedPulling="2025-01-03 20:08:32.170947457 +0000 UTC m=+33.596495332" observedRunningTime="2025-01-03 20:08:32.898897039 +0000 UTC m=+34.324444941" watchObservedRunningTime="2025-01-03 20:08:32.915665492 +0000 UTC m=+34.341213363"
Jan 03 20:08:34 minikube kubelet[2427]: I0103 20:08:34.352831    2427 topology_manager.go:215] "Topology Admit Handler" podUID="fb94de39-de80-4239-b638-7f46821907e9" podNamespace="default" podName="prometheus-alertmanager-0"
Jan 03 20:08:34 minikube kubelet[2427]: I0103 20:08:34.457307    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6x9zk\" (UniqueName: \"kubernetes.io/projected/fb94de39-de80-4239-b638-7f46821907e9-kube-api-access-6x9zk\") pod \"prometheus-alertmanager-0\" (UID: \"fb94de39-de80-4239-b638-7f46821907e9\") " pod="default/prometheus-alertmanager-0"
Jan 03 20:08:34 minikube kubelet[2427]: I0103 20:08:34.457404    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config\" (UniqueName: \"kubernetes.io/configmap/fb94de39-de80-4239-b638-7f46821907e9-config\") pod \"prometheus-alertmanager-0\" (UID: \"fb94de39-de80-4239-b638-7f46821907e9\") " pod="default/prometheus-alertmanager-0"
Jan 03 20:08:34 minikube kubelet[2427]: I0103 20:08:34.457440    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-b8b2e3dc-9685-4bf6-bba8-efb316ae2a7f\" (UniqueName: \"kubernetes.io/host-path/fb94de39-de80-4239-b638-7f46821907e9-pvc-b8b2e3dc-9685-4bf6-bba8-efb316ae2a7f\") pod \"prometheus-alertmanager-0\" (UID: \"fb94de39-de80-4239-b638-7f46821907e9\") " pod="default/prometheus-alertmanager-0"
Jan 03 20:08:35 minikube kubelet[2427]: I0103 20:08:35.006723    2427 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2f9fa2ddf1f98f57198149f7ffc901cf5e3276190d141c6483cdb392454a934f"
Jan 03 20:08:36 minikube kubelet[2427]: I0103 20:08:36.360194    2427 topology_manager.go:215] "Topology Admit Handler" podUID="619b9496-d9ab-425b-a5db-57e75c649418" podNamespace="default" podName="prometheus-server-9976468d9-klkb2"
Jan 03 20:08:36 minikube kubelet[2427]: I0103 20:08:36.473702    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/619b9496-d9ab-425b-a5db-57e75c649418-config-volume\") pod \"prometheus-server-9976468d9-klkb2\" (UID: \"619b9496-d9ab-425b-a5db-57e75c649418\") " pod="default/prometheus-server-9976468d9-klkb2"
Jan 03 20:08:36 minikube kubelet[2427]: I0103 20:08:36.473835    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-591e3c6a-f30a-422a-ab52-0d140ab8d9c7\" (UniqueName: \"kubernetes.io/host-path/619b9496-d9ab-425b-a5db-57e75c649418-pvc-591e3c6a-f30a-422a-ab52-0d140ab8d9c7\") pod \"prometheus-server-9976468d9-klkb2\" (UID: \"619b9496-d9ab-425b-a5db-57e75c649418\") " pod="default/prometheus-server-9976468d9-klkb2"
Jan 03 20:08:36 minikube kubelet[2427]: I0103 20:08:36.473900    2427 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6vxz2\" (UniqueName: \"kubernetes.io/projected/619b9496-d9ab-425b-a5db-57e75c649418-kube-api-access-6vxz2\") pod \"prometheus-server-9976468d9-klkb2\" (UID: \"619b9496-d9ab-425b-a5db-57e75c649418\") " pod="default/prometheus-server-9976468d9-klkb2"
Jan 03 20:08:37 minikube kubelet[2427]: I0103 20:08:37.057598    2427 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1fe69de400bb146439c0144b68f359484419eed1fe497bd72359464b34b2cbe2"
Jan 03 20:08:40 minikube kubelet[2427]: I0103 20:08:40.121037    2427 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/prometheus-prometheus-pushgateway-7d7b6497cd-mnl6z" podStartSLOduration=2.676921326 podStartE2EDuration="25.121008352s" podCreationTimestamp="2025-01-03 20:08:15 +0000 UTC" firstStartedPulling="2025-01-03 20:08:17.007384346 +0000 UTC m=+18.432932212" lastFinishedPulling="2025-01-03 20:08:39.451471363 +0000 UTC m=+40.877019238" observedRunningTime="2025-01-03 20:08:40.120750841 +0000 UTC m=+41.546298722" watchObservedRunningTime="2025-01-03 20:08:40.121008352 +0000 UTC m=+41.546556234"
Jan 03 20:08:46 minikube kubelet[2427]: I0103 20:08:46.244079    2427 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/prometheus-kube-state-metrics-8ff4967cb-stqs9" podStartSLOduration=3.818757226 podStartE2EDuration="31.244051136s" podCreationTimestamp="2025-01-03 20:08:15 +0000 UTC" firstStartedPulling="2025-01-03 20:08:17.039975206 +0000 UTC m=+18.465523071" lastFinishedPulling="2025-01-03 20:08:44.465269109 +0000 UTC m=+45.890816981" observedRunningTime="2025-01-03 20:08:46.243591855 +0000 UTC m=+47.669139845" watchObservedRunningTime="2025-01-03 20:08:46.244051136 +0000 UTC m=+47.669599011"
Jan 03 20:08:54 minikube kubelet[2427]: I0103 20:08:54.377927    2427 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/prometheus-alertmanager-0" podStartSLOduration=21.939097021 podStartE2EDuration="39.377906463s" podCreationTimestamp="2025-01-03 20:08:15 +0000 UTC" firstStartedPulling="2025-01-03 20:08:35.052726053 +0000 UTC m=+36.478273921" lastFinishedPulling="2025-01-03 20:08:52.489545071 +0000 UTC m=+53.917083363" observedRunningTime="2025-01-03 20:08:54.377788825 +0000 UTC m=+55.805327124" watchObservedRunningTime="2025-01-03 20:08:54.377906463 +0000 UTC m=+55.805444754"
Jan 03 20:09:19 minikube kubelet[2427]: I0103 20:09:19.765197    2427 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/prometheus-server-9976468d9-klkb2" podStartSLOduration=25.790340619 podStartE2EDuration="1m4.765172614s" podCreationTimestamp="2025-01-03 20:08:15 +0000 UTC" firstStartedPulling="2025-01-03 20:08:37.097755902 +0000 UTC m=+38.523303770" lastFinishedPulling="2025-01-03 20:09:16.070597441 +0000 UTC m=+77.498135765" observedRunningTime="2025-01-03 20:09:19.764959896 +0000 UTC m=+81.194456470" watchObservedRunningTime="2025-01-03 20:09:19.765172614 +0000 UTC m=+81.194669189"


==> storage-provisioner [05662dae4464] <==
I0103 20:08:14.880326       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0103 20:08:14.886084       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority


==> storage-provisioner [2f84b013c416] <==
I0103 20:08:31.275702       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0103 20:08:31.283352       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0103 20:08:31.283407       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0103 20:08:31.328392       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0103 20:08:31.328625       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_2bb67754-208c-4b7a-a48b-f5ce72d11ff1!
I0103 20:08:31.328675       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"6e15056c-e511-41a8-a86c-1144dbb75f96", APIVersion:"v1", ResourceVersion:"562", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_2bb67754-208c-4b7a-a48b-f5ce72d11ff1 became leader
I0103 20:08:31.429045       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_2bb67754-208c-4b7a-a48b-f5ce72d11ff1!
I0103 20:08:31.429326       1 controller.go:1332] provision "default/storage-prometheus-alertmanager-0" class "standard": started
I0103 20:08:31.429379       1 controller.go:1332] provision "default/prometheus-server" class "standard": started
I0103 20:08:31.429912       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"prometheus-server", UID:"591e3c6a-f30a-422a-ab52-0d140ab8d9c7", APIVersion:"v1", ResourceVersion:"447", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/prometheus-server"
I0103 20:08:31.429948       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"storage-prometheus-alertmanager-0", UID:"b8b2e3dc-9685-4bf6-bba8-efb316ae2a7f", APIVersion:"v1", ResourceVersion:"522", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/storage-prometheus-alertmanager-0"
I0103 20:08:31.429549       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    acfce8a7-2fc2-4b31-bc39-b023abe4ce50 320 0 2025-01-03 20:08:00 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2025-01-03 20:08:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-591e3c6a-f30a-422a-ab52-0d140ab8d9c7 &PersistentVolumeClaim{ObjectMeta:{prometheus-server  default  591e3c6a-f30a-422a-ab52-0d140ab8d9c7 447 0 2025-01-03 20:08:15 +0000 UTC <nil> <nil> map[app.kubernetes.io/component:server app.kubernetes.io/instance:prometheus app.kubernetes.io/managed-by:Helm app.kubernetes.io/name:prometheus app.kubernetes.io/part-of:prometheus app.kubernetes.io/version:v3.1.0 helm.sh/chart:prometheus-26.1.0] map[meta.helm.sh/release-name:prometheus meta.helm.sh/release-namespace:default volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{helm Update v1 2025-01-03 20:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:meta.helm.sh/release-name":{},"f:meta.helm.sh/release-namespace":{}},"f:labels":{".":{},"f:app.kubernetes.io/component":{},"f:app.kubernetes.io/instance":{},"f:app.kubernetes.io/managed-by":{},"f:app.kubernetes.io/name":{},"f:app.kubernetes.io/part-of":{},"f:app.kubernetes.io/version":{},"f:helm.sh/chart":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2025-01-03 20:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/prometheus-server
I0103 20:08:31.429629       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    acfce8a7-2fc2-4b31-bc39-b023abe4ce50 320 0 2025-01-03 20:08:00 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2025-01-03 20:08:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-b8b2e3dc-9685-4bf6-bba8-efb316ae2a7f &PersistentVolumeClaim{ObjectMeta:{storage-prometheus-alertmanager-0  default  b8b2e3dc-9685-4bf6-bba8-efb316ae2a7f 522 0 2025-01-03 20:08:15 +0000 UTC <nil> <nil> map[app.kubernetes.io/instance:prometheus app.kubernetes.io/name:alertmanager] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2025-01-03 20:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app.kubernetes.io/instance":{},"f:app.kubernetes.io/name":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/storage-prometheus-alertmanager-0
I0103 20:08:31.430747       1 controller.go:1439] provision "default/prometheus-server" class "standard": volume "pvc-591e3c6a-f30a-422a-ab52-0d140ab8d9c7" provisioned
I0103 20:08:31.430790       1 controller.go:1456] provision "default/prometheus-server" class "standard": succeeded
I0103 20:08:31.430798       1 volume_store.go:212] Trying to save persistentvolume "pvc-591e3c6a-f30a-422a-ab52-0d140ab8d9c7"
I0103 20:08:31.430957       1 controller.go:1439] provision "default/storage-prometheus-alertmanager-0" class "standard": volume "pvc-b8b2e3dc-9685-4bf6-bba8-efb316ae2a7f" provisioned
I0103 20:08:31.430994       1 controller.go:1456] provision "default/storage-prometheus-alertmanager-0" class "standard": succeeded
I0103 20:08:31.430999       1 volume_store.go:212] Trying to save persistentvolume "pvc-b8b2e3dc-9685-4bf6-bba8-efb316ae2a7f"
I0103 20:08:31.449224       1 volume_store.go:219] persistentvolume "pvc-b8b2e3dc-9685-4bf6-bba8-efb316ae2a7f" saved
I0103 20:08:31.449356       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"storage-prometheus-alertmanager-0", UID:"b8b2e3dc-9685-4bf6-bba8-efb316ae2a7f", APIVersion:"v1", ResourceVersion:"522", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-b8b2e3dc-9685-4bf6-bba8-efb316ae2a7f
I0103 20:08:31.450025       1 volume_store.go:219] persistentvolume "pvc-591e3c6a-f30a-422a-ab52-0d140ab8d9c7" saved
I0103 20:08:31.450112       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"prometheus-server", UID:"591e3c6a-f30a-422a-ab52-0d140ab8d9c7", APIVersion:"v1", ResourceVersion:"447", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-591e3c6a-f30a-422a-ab52-0d140ab8d9c7

